---
title: "Weekly update"
output: html_document
date: "2022-11-15"
---

## Nov 14-18, 2022

**Meeting on Nov 15**

* GitHub setup, cloned repository
* Discussed about project background and details
* Discussed next steps
  + prepare for the presentation
  + review *Applied Longitudinal Analysis* (Chapter 11-13)
* Q&A:
  + What's the difference between GEE and GLMM?
    - The interpretation of the parameters is different. The target of inference of GEE is population-average, whereas the GLMM is subject-specific.
    
**Accomplishment**

1. Literature Review
a) Propensity Score methods
b) Generalized Estimating Equations (GEE)
c) Time-dependent Covariates (12.1-12.3)
d) Cardiovascular research
  * Improved Outcomes Following the Ross Procedure Compared With Bioprosthetic Aortic Valve Replacement
e) review Generalized linear model
  
2. Presentation slides (draft)

3. On-boarding process
* completed the UHN Office of Research Trainee(ORT) e-registration form
* returned a signed offer of appointment, Personal Data and Emergency Contact Form and documents from the part A of appendix A(passport & study permit)
* Start date: December 5, 2022
* Orientation: Tuesday, December 13, 2022

**Q&A**

1. Why GEE with independent working correlation is used for unbiased estimation when the follow-up data is *truncated by death*?
- censoring: assume independent, missing not at random
- GEE: assume missing completely at random. 
- Inverse probability weighting: dealing with drop-out due to death
- For this practicum, assume we don't have missing data.
2. What is the purpose of simulations before using the real data for this study?
- Logistic problem: we don't have the data yet
- Need real data to design the simulation study and decide what parameter we're going to use. We want the simulated data as close as the real data. We'll use the real data to do analysis(application/illustration), but won't use it to make any new scientific discoveries.
3. How  do we include two correlation matrices for the matched pairs and the longitudinal responses while performing GEE?
4. What is the robust sandwich estimator? 
- solved: section 13.2 from Applied Longitudinal Analysis, 
$Cov(\hat{\beta}) = B^{-1}MB^{-1}$
- can obtain valid standard error for $\hat{\beta}$ under the misspecified model for the within-subject association, with large sample sizes
- robust sandwich estimator VS model-based estimator: generally use the robust sandwich estimator, almost never use the other one (unless if we are sure the correlation structure is correcly specified)


## Nov 21-25, 2022
**Meeting on Nov 22**

* Discussed questions that I had from last week
* Modified presentation slides
* Discussed next steps:
  + read case studies in Chapter 13
  + use Arthritis Clinical Trial data to perform GEE 

**Accomplishment**

* Presentation preparation

* Arthritis Clinical Trial example:
  + transformed the data into long format using "pivot_longer"
  + follow the sample code and fit a marginal proportional odds(ordinal) regression model with independent working correlation matrix
  + dichotomize the response variable, and fit a Marginal Logistic Regression Model with independent working correlation matrix
  
* On-boarding process:
  + UHN network account access information(TID): t119634uhn
  + Employee ID: 538335
  + UHN email address: Peiyu.Li@uhn.ca
  + Pick up computer on December 8
  
**Q&A**

1. In the case study Arthritis Clinical Trial in Chapter 13:
  a) Why the model used square-root transformation for time(month)?
    + month(0,2,4,6) --> (0,1.4,2,2,45), after square-root transformation, the difference is getting smaller as time increases. Instead of assuming linear effect, we assume the difference on treatment effect is largest for the first visit, and smallest for the last visit.
  b) When I changed the correlation structure in the model from independent to unstructured, the estimates in the output became "NaN". What other changes I should make to avoid this issue?
    + probably because different R version
    
2. Dichotomous response variable:
  Why the sample code uses *ordgee()* function with logit link to fit both Marginal Logistic Regression Model and Marginal Proportional Odds (Ordinal) Regression Model? Since the interpretation of the parameters is quite different for these two models.
  geeglm() gives same result as ordgee() with logit link. Are there any differences in using these two functions?
   + $\alpha$ in ordgee() output provides estimate of the *odds ratio parameter*.
   + $\alpha$ in geeglm() output provides estimate of the *correlation parameters*, and $-1 \leq \alpha \leq 1$.

  
  
## Nov 28 - Dec 02, 2022
**Meeting on Nov 29**

1. R function options:
  - waves: Variable specifying the ordering of repeated measurements on the same unit (tells R the dimension of correlation matrix). Also used in connection with missing values. 
  - scale.fix: If true, it's fix at the value of 'scale.value'(by default at 1) rather than estimated. Scale is 1 for logistic regression.
2. Next steps:
  - Use arthritis dataset (303 individuals), compare parameter coefficients, standard errors, and QIC for each model using different correlation structure.
  - Randomly select 50 and 20 individuals, do the comparison again.
  
**Accomplishment**

* End of term presentation on Dec 1
* Completed Health Services mandatory immunization requirements
* Completed UHN mandatory trainings

## Dec 05 - Dev 09, 2022
**No Meeting (Exam Week)**

**Accomplishement**

* Picked up computer, met with Kate and Sudip
* UHN orientation
* Made comparison tables for different GEE regressions using four correlation structures and different sample sizes.

**Q&A**

1. If the numbers of repeated measurements are not the same (e.g. contain missing values), how can we specify "waves" parameter in geeglm()? Should I specify "waves" parameter as a list of numbers of observations for each subject?
* The missing values should not affect the results when we specified "waves=time". * GEE assumes data are missing completely at random.

2. 
a) When should we set scale.fix to be false and allow the scale to be estimated from the data? 
 + In GLM, there is a *location* parameter and *scale* parameter. For logistic regression, location parameter is $\mu = P(Y=1)$ and scale is 1 because $Var=\mu(1-\mu)$ only relies on the location parameter. 
 + If we have a continuous outcome assume a Normal distribution, location parameter is mean $\mu$, the scale parameter equals $Var=\sigma^2$. In this case, we won't fix the scale parameter and we need to estimate it from the data.
 + So it depends on which distribution we're assuming.

b) Why the scale parameter becomes 1.01 instead of 1 when only included 20 subjects?
  + When using the geeglm() function in R with a small sample size and scale.fix=TRUE, the estimated scale parameter may not be exactly equal to 1. 

3. I used geepack::geeglm function for the comparison, and we can directly use QIC function. However, gee:gee function can provide both naive standard errors and robust standard error, and we can also get estimated correlation structure matrix in the summary output for gee() function. 
* The estimates are a little bit different. gee() uses moment based approach, geeglm() constructs another estimating equation for the $\alpha 's$ and iterate between two estimating equations.
* --> both use moment-based approach, but with different optimization algorithm.

4. How to interpret the results based on the comparisons?
* The effect of 'sqrtmonth' became insignificant when the sample size decreased
* standard errors for time-related variables increased as the sample size decreased.
* QIC for different correlation structured were almost the same for a given sample size, but QIC decreased as the sample size decreased.

5. For propensity score matching, do we need to consider caliper distance when performing the matching?
* Yes.

## Dec 12, 2022 - Jan 06, 2023 (winter break)
**Meeting on Dec13**

* Discussed and compared the results from the comparison tables using different working correlation structures and sample sizes (Arthritis clinical trial data).
* Discussed questions I had when working on the comparison exercise.
* Other important things we mentioned: 
  + Extracting QICu instead of QIC might be better. AR(1) has to estimate $\rho$ or $\alpha$ parameter but independence doesn't, so there's going to be imbalance between number of parameters that each model is estimating.
  + bal_tab(): *standardized difference* close to 0 shows balance. 
  + Don't use p-value to check balance. Some people use P-value to check if the mean age between treated and untreated group are not different. That's not what we're trying to investigate. P-value is the probability that you observed a difference given there's a true difference. We don't want to assume that there is a true difference.
  + How different we want they to be depends on clinical questions. We want to create a sample that mimics a randomized clinical trial. Even if we randomized patients perfectly, we might see some different in baseline characteristics between treated and untreated groups. 
  + AR(1) correlation is $\rho^{|t_i-t_j|}$, so it decreases as the time lag increases.
  + the *linear interaction* is not significant, if the interaction is not linear
* Next steps:
  + follow up with Steve and Sudip, and practice using R in docker.
  + plot the longitudinal data: for binary outcome, we can plot the proportion of the outcome at each time point for each treatment (one curve for each treatment). If the outcome increase or decrease in a linear matter, then we can just include the linear time variable.
  + Get familiar with QLS
  

**Accomplishment**

* Followed up with Sudip:
  + A new data set, still waiting for approval
  + Issue on R in docker: 
    - In-person meeting with Sudip on Thursday (Jan 12)
    -  It's possible that everyone else using Steve’s WSL-docker set-up is doing it on a PC that sits on the network at UHN, and they RDP (remote desktop) to it. I, on the other hand, have a laptop at home that is connecting via VPN.

* Plot the arthritis longitudinal data
* Read the papers and learned QLS(2 stages):
  + Stage 1: Alternates between updating estimates of the regression parameters(via the GEE estimating equation for $\beta$) and of the correlation parameters($\alpha$, by minimizing generalized error sum of square with respect to $\alpha$) until convergence.
  + Stage 2: Obtain a consistent estimate as the solution for the true correlation parameter $\hat \rho$ (stage 2 estimates of $\alpha$) by solving the stage 2 estimating equation, assuming the working structure is correctly specified.
  + The final estimate of $\beta$ is obtained by solving the GEE estimating equation evaluated at $\hat \rho$.
  
  
**Q&A**

1. In our study, are the correlations between matched pairs considered "intra-cluster" correlations?
--> Yes, each pair forms a cluster, and each individual is also a cluster.

2. GEE vs QLS?
  + GEE typically implements moment estimates of the correlation parameter, while QLS obtains solutions to estimating equations.
  + Use the same estimating equation to estimate $\beta$
  + QLS is based on GEE, but more easily applied for some complex correlation structures
  + When the number of cluster is small, GEE(sandwich estimator) tends to *underestimate SEs* (Bie et al., 2020).  Does it apply to QLS as well? --> Yes
  + QLS estimation is more efficient and superior than GEE estimation. (Asar & Karabulut, 2021)
  
3. In our study, to simultaneously account for two sources of correlation, are we going to use *the Kronecker product of two correlation matrices*? (Shults & Morrow, 2002; Shults et al., 2004)
--> Yes. See Mitani et al. (2019).
  
4. The authors compared efficiency of the regression parameter estimates to demonstrate the effect of misspecification of the working correlation structure, are we also going to calculate and compare the efficiency of parameter estimates?
--> Yes, we'll calculate both bias and efficiency.

5. Does QIC applicable to QLS?


## Jan 09-13, 2023
**Meeting on Jan 10 **

* Arthritis clinical trial data: Based on the plot, consider quadratic effect on time. Model: trt + month + month^2 + trt: month + trt:month^2
* Extended GEE methods:
  + GEE1(gee::gee): $\beta$ is estimated using GEE estimating equation, $\alpha$ is estimated using *moment based approach*.
  + GEE1.5: $\beta$ & $\alpha$ estimated by estimating equations, iterate between the two
  + GEE2(geepack::geeglm): estimated by joint estimating equation, solve matrix $\eta = (\beta, \alpha)$
* The R package "qlspack" is no longer available, and we need to write our own code.
* "Quasi-Least Squares Regression" by Justine Shults
* Next time, we'll solve generalized sum of squares together. 

**Accomplishment**

1) Simulation practice
* run a simulation for simple linear regression, with two parameters $\beta_0$ and $\beta_1$, where the predictor follows Bin(N,p) and random error follows N(0,1).
* Calculate mean relative bias, mean SE, SD(empirical SE), coverage probability, and MSE for $\beta_0$ and $\beta_1$.
* see "Simulation Practice.Rmd"

2) Practicum data
- Steve provided partial data dictionary, and I also confirmed the definition of some other variables with him.

* Dataset 1 ("work_pt_d")
  + contains the baseline (i.e. patient-level) information of 406 patients (variables that are included here could be utilized for matching)  
  
|Variable |Description                                                             |
|:--------|:-----------------------------------------------------------------------|
|ptid     |Patient ID                                                              |
|age      |Patient's age                                                           |
|sex      |Patient's sex, 1_male vs 2_female                                       |
|bav      |Bicuspid aortic valve (BAV) vs Tricuspid  aortic vlave (TAV)            |
|nc_sinus |Replacement of the non-coronary sinus of the aortic root (1_Yes vs 2_No)|
|raa_type |Type of ascending aorta replacement: (1_clamp vs 2_hemiarch)            |
|bsa      |Body Surface Area                                                       |
|diabetes |0_Negative vs 1_Diabetes                                                |
|hyper    |0_Negative vs 1_Hypertension                                            |
|chlstrl  |0_Negative vs 1_hyperlipidemia                                          |

* Dataset 2 ("raw_long_d")
  + contains the corresponding longitudinal measurements of these patients (298 patients)

|Variable     |Description                                                                         |
|:------------|:-----------------------------------------------------------------------------------|
|ptid         |Patient ID                                                                          |
|age          |Patient's age                                                                       |
|sex          |Patient's sex, 1_male vs 2_female                                                   |
|bav_confirmed|BAV vs TAV                                                                          |
|nc_sinus     |Replacement of the non-coronary sinus of the aortic root (1_Yes vs 2_No)            |
|raa_type     |Type of ascending aorta replacement: (1_clamp vs 2_hemiarch)                        |
|died         |Patient died, 0 = No, 1 = Yes                                                       |
|lka_d        |Date of last known alive                                                            |
|yr2death     |Year until death                                                                    |
|ao_reop      |Repeat aortic surgery (0: no reop and alive, 1: reop and alive, 2: reop and died)   |
|yr2ao_reop   |Year until re-operation                                                             |
|bsa_baseline |Body surface area at the baseline                                                   |
|dateor       |Date of operation(baseline date)                                                    |
|bsa_echo     |body surface area measured at echo dates post baseline (i.e. follow up BSA measures)|
|mdate        |Measurement date                                                                    |
|root         |Aortic root size                                                                    |
|aa           |Ascending aorta size                                                                |
|arch         |Aortic arch size                                                                    |
|src_root     |1 = ECHO, 2 = CT, 3 = MRI                                                           |
|src_aa       |1 = ECHO, 2 = CT, 3 = MRI                                                           |
|src_arch     |1 = ECHO, 2 = CT, 3 = MRI                                                           |
|day2vst      |Number of days from "dateor" to "mdate"                                             |
|yr2vst       |Number of years from "dateor" to "mdate"                                            |
|date_base    |Date of the first measurement                                                       |

* Note: for "day2vst" and "yr2vst", 0 means measured on the day of operation, negatice values indicate meaured before date of operation.  
* Outcome variables: root, ascending aorta, and arch dimension (focus on root dimension first).    
  + we can exclude the missing values. 
* Cohort variable: bicuspid aortic valve (BAV) vs Tricuspid  aortic valve (TAV). 


**Q&A**

1. For the variable "day2vst"/"yr2vst", should we exclude observations that have negative values on this variable? (i.e. should we only include measurements that took place after operation?) --> Yes.

2. Some patients had repeated aortic surgery. We need to compare the measurement date("mdate") and re-operation date, then exclude observations that measured the outcomes after re-operation.
  + check "yr2ao_reop" vs "yr2vst" (i.e. for those who did re-operation, year until measurement should be less than year until re-operation) ?  --> Yes.

3. Steve mentioned that we should pay attention to the follow-up time. For each patient, if one of the measurement took place far away from other repeated measurement, then we may need to exclude this observation.
  + How to visualize this?
  + we can calculate the time difference between measurements, but how to decide the standard of excluding observations due to large time difference?

4. Imaging data were collected from all available *echocardiography*, *CT*, and *Magnetic resonance imaging(MRI)* reports at baseline and during follow-up.
  + Sudip said these may not be relevant for our project, so we can ignore those variables for now.
  + However, Steve wanted me to discuss with you about the selection bias (which will translate into the bias within parameters). In reality, we cannot quantify selection bias. During the simulation (matching), we can simulate 2 datasets (one was measured using only ECHO, the other one was measured from 3 different sources). Then we can add some bias to the parameter to simulate the bias.
  + The question is: To what extend the bias should be introduced so that the result from the former dataset will approach the later one. If there's only a little bit study based bias, but translate to huge parameter bias, then that's the concern. But if we induce large selection bias in the study, and only observe relatively small parameter related bias, then he'll be more comfortable to use that.
--> can be discussed later

## Jan 16-20, 2023
**Meeting on Jan 17 **

* "ao_reop": Repeat aortic surgery (0: no reop and alive, 1: reop and alive, 2: reop and died)
* "dateor": baseline date
* We need to define our sample first, before doing visualization for the outcome variable (eg. histogram of year between visits, skewed?)
* Next step:
  + count the number of observations each patient has, and create a new variable "visit" and "total visit" for each patient
  + total visits vs frequency
  + create a table compare characteristics of patients with and without baseline measurement
  + other data exploration
  + Read *Quasi-Least Squares* CH2(review) and CH3
  
**Accomplishment**

* Read *Quasi-Least Squares Regression* CH2, and still reading CH3
* Exploratory Data Analysis
  * Merged baseline variables to the longitudinal dataset
  * Create variables **visit** and **total visit** to record number of measurements that each patients had.
  * Analyze baseline information for patients with longitudinal measurements (298/406)                     + Hmisc::describe() to see basic summary statistics for all variables
    + mean **age** around 64, 68% of the patients are **males**
    + **BAV** 70%(209) vs **TAV** 30%(89)
  * Summary for other variables
    + **mdate**: the earliest measurement date is in 1994
    + 3 abnormal root dimension (lowest 3 measurements), 148 missing measurements
    + **src_root**: 98% of measurements used the same method, so the measurement error should be limited
    + **yr2vst**(or day2vst): some extremely high values eg. 19 years since 1st operation date
  * Baseline measurement (**"date_base"**): the measurements that occurred before/on the day of 1st operation were considered as baseline measurement of outcome.
    + 38% patients did not have baseline measurements. 
  * Create a new variable "yr2vst_diff" to record the time difference between consecutive measurements for each patient. (But it didn't consider those without baseline measurement who had their 1st measurement a long time after their 1st operation)

**Q&A**

1.  Recall "ao_reop" (0: no reop and alive, 1: reop and alive, 2: reop and died, but there's a patient whose ao_reop = 1 but died? 
  + Re-categorize this patient to ao_reop = 2 for now.
  
2. The earliest (date_base) measurement date is 1994, but there's still 38% of the patients who did not have baseline measurements. 
  + We'll select our sample for patients who had a baseline measurement and at least 2 total visits.

3.  How can we determine the cut-off point for excluding measurements that were taken a long time after the previous measurement(for those with baseline measurements) or 1st operation(for those without baseline measurements) ?
  + First we need to select our sample.
  
  
## Jan 23-27, 2023
**Meeting on Jan 24**

* Possible questions for Steve: How to dichotomize the response variable?
* Select sample based on two conditions: 
  + 1st visit should occurred on the date of 1st operation
  + at least 2 total visits(including the baseline measurement)
* Create a table to check the balance of baseline characteristics between BAV and TAV group
* Match the selected sample
* Re-do the plots

**Accomplishment**

* Read Quasi-Least Squares Regression Chapter3
* Selected sample based on the two conditions
* Performed pair matching base on all the baseline characteristics for the selected sample, resulting 25 pairs of patients
* Check the balance between two groups before & after matching
* Create several figures
  + distribution of root at baseline & mean root change over time by groups
  + distribution of yr2vst by groups

**Q&A**

1. The baseline characteristics are not very balance after matching (especially for the propensity score,age,sex,nc_sinus). Consider drop some covariates like nc_sinus(Replacement of the non-coronary sinus of the aortic root)?   
* --> Use only age, sex, and bsa.

2. If we look at the histogram of yr2vst, their are some patients with TAV that had yr2vst greater than 10 years. However, the visit time difference between 2 consecutive visit are all less than 10 years. So the visit time is reasonable and not too far away from the last visit, but might be a little too far away from the 1st operation for some patients. 

3. For patients with tricuspid aortic valve(TAV), they seem to had larger range of root dimension and year to visit values.
  + People with bicuspid aortic valves (BAVs) are at a higher risk of developing aortic valve disease compared to those with TAV. So if people who had TAV and still need to do the operation to replace their aortic valve, does it mean those patients had more severe condition(extremely small or large root/older/diabetes/hyper/chlstrl) and that's probability the reason why they had larger range of root dimension?

4. Since the follow up time for each patient had unequal time intervals, does it mean the AR(1) correlation structure might not capture the within-subject correlation well?  
* --> Yes. Since the data is not balance and measurements are mistimed, that's going to be a problem when we try to specify a covariance structure. A lot of structures assume balanced data, like AR(1), Toeplitz, and even the unstructured correlation structure.


## Jan 30 - Feb 03, 2023
**Meeting on Jan 31**

1. Data
* The data is not balanced, and measurements are mistimed. e.g. The time between visit 1 and visit 2 for 2 people are going to be very different.
* Next step: 
  + Simplify the data: delete observations greater than or equal to 10 visits
  + Fit a GEE model with independent correlation structure, using BAV_confirmed, visit, BAV_confirmed*visit as predictors, and root as outcome variable.

2. Exercise
* Try to solve the generalized sum of square with respect to $\alpha$.

**Accomplishment**

* Fit a GEE model with independent correlation structure.
* Derived the equation for $\hat \alpha$ from $Q(\alpha, \beta)$.
* Attend the Intro to Niagara class.

**Questions**
  
1. Why we only need one summation for i, what about j and k? 

2. After getting the derivative of $R_{ij}^{-1}(\alpha)$, how the equation 
$$\sum_{i=1}^N Z_i' \frac{\partial R_i^{-1}}{\partial \alpha} Z_i $$
is equivalent to 
$$\sum_{i=1}^N [\alpha S_1 - (1+\alpha^2) S_2] $$

3. When fitting the GEE model, I set waves = visit. The max(visit) is 9 but each patients might have different number of visit, does it matter? (family=gaussian for continuous variable?)

4. Niagara
  + Are we going to use R modules? Which R version should I load? Packages?
  + How to use it?
  + my account expires on May 10, 2023?


**No Meeting on Feb 7**


## Feb 13-17, 2023

**No meeting on Feb 14**

**Next Steps**

1. Play around with the package CWGEE that was developed as part of the paper on ordinal cluster weighted GEE. (https://github.com/ayamitani/CWGEE)
2. Run the main function
3. Go into the specific R scripts that computes each step. For example, the script names “estalpha1_ar1.R” computes the alpha estimates in AR1 from the derivation that you just went through.

## Feb 20-24, 2023

**Reading Week(No meeting)**

**Accomplishment**

1. Get familiar with the package CWGEE, tried the examples provided in the "README" file using the functions "ordCWGEE" and "ordCWGEE".
2. Understand the R script "estalpha1_ar1" based on the calculation i did last week.



## Feb 27- Mar 3, 2023













