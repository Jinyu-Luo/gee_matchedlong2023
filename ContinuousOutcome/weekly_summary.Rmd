---
title: "Weekly update"
output: html_document
date: "2022-11-15"
---

## Nov 14-18, 2022

**Meeting on Nov 15**

* GitHub setup, cloned repository
* Discussed about project background and details
* Discussed next steps
  + prepare for the presentation
  + review *Applied Longitudinal Analysis* (Chapter 11-13)
* Q&A:
  + What's the difference between GEE and GLMM?
    - The interpretation of the parameters is different. The target of inference of GEE is population-average, whereas the GLMM is subject-specific.
    
**Accomplishment**

1. Literature Review
a) Propensity Score methods
b) Generalized Estimating Equations (GEE)
c) Time-dependent Covariates (12.1-12.3)
d) Cardiovascular research
  * Improved Outcomes Following the Ross Procedure Compared With Bioprosthetic Aortic Valve Replacement
e) review Generalized linear model
  
2. Presentation slides (draft)

3. On-boarding process
* completed the UHN Office of Research Trainee(ORT) e-registration form
* returned a signed offer of appointment, Personal Data and Emergency Contact Form and documents from the part A of appendix A(passport & study permit)
* Start date: December 5, 2022
* Orientation: Tuesday, December 13, 2022

**Q&A**

1. Why GEE with independent working correlation is used for unbiased estimation when the follow-up data is *truncated by death*?
- censoring: assume independent, missing not at random
- GEE: assume missing completely at random. 
- Inverse probability weighting: dealing with drop-out due to death
- For this practicum, assume we don't have missing data.
2. What is the purpose of simulations before using the real data for this study?
- Logistic problem: we don't have the data yet
- Need real data to design the simulation study and decide what parameter we're going to use. We want the simulated data as close as the real data. We'll use the real data to do analysis(application/illustration), but won't use it to make any new scientific discoveries.
3. How  do we include two correlation matrices for the matched pairs and the longitudinal responses while performing GEE?
4. What is the robust sandwich estimator? 
- solved: section 13.2 from Applied Longitudinal Analysis, 
$Cov(\hat{\beta}) = B^{-1}MB^{-1}$
- can obtain valid standard error for $\hat{\beta}$ under the misspecified model for the within-subject association, with large sample sizes
- robust sandwich estimator VS model-based estimator: generally use the robust sandwich estimator, almost never use the other one (unless if we are sure the correlation structure is correcly specified)


## Nov 21-25, 2022
**Meeting on Nov 22**

* Discussed questions that I had from last week
* Modified presentation slides
* Discussed next steps:
  + read case studies in Chapter 13
  + use Arthritis Clinical Trial data to perform GEE 

**Accomplishment**

* Presentation preparation

* Arthritis Clinical Trial example:
  + transformed the data into long format using "pivot_longer"
  + follow the sample code and fit a marginal proportional odds(ordinal) regression model with independent working correlation matrix
  + dichotomize the response variable, and fit a Marginal Logistic Regression Model with independent working correlation matrix
  
* On-boarding process:
  + UHN network account access information(TID): t119634uhn
  + Employee ID: 538335
  + UHN email address: Peiyu.Li@uhn.ca
  + Pick up computer on December 8
  
**Q&A**

1. In the case study Arthritis Clinical Trial in Chapter 13:
  a) Why the model used square-root transformation for time(month)?
    + month(0,2,4,6) --> (0,1.4,2,2,45), after square-root transformation, the difference is getting smaller as time increases. Instead of assuming linear effect, we assume the difference on treatment effect is largest for the first visit, and smallest for the last visit.
  b) When I changed the correlation structure in the model from independent to unstructured, the estimates in the output became "NaN". What other changes I should make to avoid this issue?
    + probably because different R version
    
2. Dichotomous response variable:
  Why the sample code uses *ordgee()* function with logit link to fit both Marginal Logistic Regression Model and Marginal Proportional Odds (Ordinal) Regression Model? Since the interpretation of the parameters is quite different for these two models.
  geeglm() gives same result as ordgee() with logit link. Are there any differences in using these two functions?
   + $\alpha$ in ordgee() output provides estimate of the *odds ratio parameter*.
   + $\alpha$ in geeglm() output provides estimate of the *correlation parameters*, and $-1 \leq \alpha \leq 1$.

  
  
## Nov 28 - Dec 02, 2022
**Meeting on Nov 29**

1. R function options:
  - waves: Variable specifying the ordering of repeated measurements on the same unit (tells R the dimension of correlation matrix). Also used in connection with missing values. 
  - scale.fix: If true, it's fix at the value of 'scale.value'(by default at 1) rather than estimated. Scale is 1 for logistic regression.
2. Next steps:
  - Use arthritis dataset (303 individuals), compare parameter coefficients, standard errors, and QIC for each model using different correlation structure.
  - Randomly select 50 and 20 individuals, do the comparison again.
  
**Accomplishment**

* End of term presentation on Dec 1
* Completed Health Services mandatory immunization requirements
* Completed UHN mandatory trainings

## Dec 05 - Dev 09, 2022
**No Meeting (Exam Week)**

**Accomplishement**

* Picked up computer, met with Kate and Sudip
* UHN orientation
* Made comparison tables for different GEE regressions using four correlation structures and different sample sizes.

**Q&A**

1. If the numbers of repeated measurements are not the same (e.g. contain missing values), how can we specify "waves" parameter in geeglm()? Should I specify "waves" parameter as a list of numbers of observations for each subject?
* The missing values should not affect the results when we specified "waves=time". * GEE assumes data are missing completely at random.

2. 
a) When should we set scale.fix to be false and allow the scale to be estimated from the data? 
 + In GLM, there is a *location* parameter and *scale* parameter. For logistic regression, location parameter is $\mu = P(Y=1)$ and scale is 1 because $Var=\mu(1-\mu)$ only relies on the location parameter. 
 + If we have a continuous outcome assume a Normal distribution, location parameter is mean $\mu$, the scale parameter equals $Var=\sigma^2$. In this case, we won't fix the scale parameter and we need to estimate it from the data.
 + So it depends on which distribution we're assuming.

b) Why the scale parameter becomes 1.01 instead of 1 when only included 20 subjects?
  + When using the geeglm() function in R with a small sample size and scale.fix=TRUE, the estimated scale parameter may not be exactly equal to 1. 

3. I used geepack::geeglm function for the comparison, and we can directly use QIC function. However, gee:gee function can provide both naive standard errors and robust standard error, and we can also get estimated correlation structure matrix in the summary output for gee() function. 
* The estimates are a little bit different. gee() uses moment based approach, geeglm() constructs another estimating equation for the $\alpha 's$ and iterate between two estimating equations.
* --> both use moment-based approach, but with different optimization algorithm.

4. How to interpret the results based on the comparisons?
* The effect of 'sqrtmonth' became insignificant when the sample size decreased
* standard errors for time-related variables increased as the sample size decreased.
* QIC for different correlation structured were almost the same for a given sample size, but QIC decreased as the sample size decreased.

5. For propensity score matching, do we need to consider caliper distance when performing the matching?
* Yes.

## Dec 12, 2022 - Jan 06, 2023 (winter break)
**Meeting on Dec13**

* Discussed and compared the results from the comparison tables using different working correlation structures and sample sizes (Arthritis clinical trial data).
* Discussed questions I had when working on the comparison exercise.
* Other important things we mentioned: 
  + Extracting QICu instead of QIC might be better. AR(1) has to estimate $\rho$ or $\alpha$ parameter but independence doesn't, so there's going to be imbalance between number of parameters that each model is estimating.
  + bal_tab(): *standardized difference* close to 0 shows balance. 
  + Don't use p-value to check balance. Some people use P-value to check if the mean age between treated and untreated group are not different. That's not what we're trying to investigate. P-value is the probability that you observed a difference given there's a true difference. We don't want to assume that there is a true difference.
  + How different we want they to be depends on clinical questions. We want to create a sample that mimics a randomized clinical trial. Even if we randomized patients perfectly, we might see some different in baseline characteristics between treated and untreated groups. 
  + AR(1) correlation is $\rho^{|t_i-t_j|}$, so it decreases as the time lag increases.
  + the *linear interaction* is not significant, if the interaction is not linear
* Next steps:
  + follow up with Steve and Sudip, and practice using R in docker.
  + plot the longitudinal data: for binary outcome, we can plot the proportion of the outcome at each time point for each treatment (one curve for each treatment). If the outcome increase or decrease in a linear matter, then we can just include the linear time variable.
  + Get familiar with QLS
  

**Accomplishment**

* Followed up with Sudip:
  + A new data set, still waiting for approval
  + Issue on R in docker: 
    - In-person meeting with Sudip on Thursday (Jan 12)
    -  It's possible that everyone else using Steve’s WSL-docker set-up is doing it on a PC that sits on the network at UHN, and they RDP (remote desktop) to it. I, on the other hand, have a laptop at home that is connecting via VPN.

* Plot the arthritis longitudinal data
* Read the papers and learned QLS(2 stages):
  + Stage 1: Alternates between updating estimates of the regression parameters(via the GEE estimating equation for $\beta$) and of the correlation parameters($\alpha$, by minimizing generalized error sum of square with respect to $\alpha$) until convergence.
  + Stage 2: Obtain a consistent estimate as the solution for the true correlation parameter $\hat \rho$ (stage 2 estimates of $\alpha$) by solving the stage 2 estimating equation, assuming the working structure is correctly specified.
  + The final estimate of $\beta$ is obtained by solving the GEE estimating equation evaluated at $\hat \rho$.
  
  
**Q&A**

1. In our study, are the correlations between matched pairs considered "intra-cluster" correlations?
--> Yes, each pair forms a cluster, and each individual is also a cluster.

2. GEE vs QLS?
  + GEE typically implements moment estimates of the correlation parameter, while QLS obtains solutions to estimating equations.
  + Use the same estimating equation to estimate $\beta$
  + QLS is based on GEE, but more easily applied for some complex correlation structures
  + When the number of cluster is small, GEE(sandwich estimator) tends to *underestimate SEs* (Bie et al., 2020).  Does it apply to QLS as well? --> Yes
  + QLS estimation is more efficient and superior than GEE estimation. (Asar & Karabulut, 2021)
  
3. In our study, to simultaneously account for two sources of correlation, are we going to use *the Kronecker product of two correlation matrices*? (Shults & Morrow, 2002; Shults et al., 2004)
--> Yes. See Mitani et al. (2019).
  
4. The authors compared efficiency of the regression parameter estimates to demonstrate the effect of misspecification of the working correlation structure, are we also going to calculate and compare the efficiency of parameter estimates?
--> Yes, we'll calculate both bias and efficiency.

5. Does QIC applicable to QLS?


## Jan 09-13, 2023
**Meeting on Jan 10 **

* Arthritis clinical trial data: Based on the plot, consider quadratic effect on time. Model: trt + month + month^2 + trt: month + trt:month^2
* Extended GEE methods:
  + GEE1(gee::gee): $\beta$ is estimated using GEE estimating equation, $\alpha$ is estimated using *moment based approach*.
  + GEE1.5: $\beta$ & $\alpha$ estimated by estimating equations, iterate between the two
  + GEE2(geepack::geeglm): estimated by joint estimating equation, solve matrix $\eta = (\beta, \alpha)$
* The R package "qlspack" is no longer available, and we need to write our own code.
* "Quasi-Least Squares Regression" by Justine Shults
* Next time, we'll solve generalized sum of squares together. 

**Accomplishment**

1) Simulation practice
* run a simulation for simple linear regression, with two parameters $\beta_0$ and $\beta_1$, where the predictor follows Bin(N,p) and random error follows N(0,1).
* Calculate mean relative bias, mean SE, SD(empirical SE), coverage probability, and MSE for $\beta_0$ and $\beta_1$.
* see "Simulation Practice.Rmd"

2) Practicum data
- Steve provided partial data dictionary, and I also confirmed the definition of some other variables with him.

* Dataset 1 ("work_pt_d")
  + contains the baseline (i.e. patient-level) information of 406 patients (variables that are included here could be utilized for matching)  
  
|Variable |Description                                                             |
|:--------|:-----------------------------------------------------------------------|
|ptid     |Patient ID                                                              |
|age      |Patient's age                                                           |
|sex      |Patient's sex, 1_male vs 2_female                                       |
|bav      |Bicuspid aortic valve (BAV) vs Tricuspid  aortic vlave (TAV)            |
|nc_sinus |Replacement of the non-coronary sinus of the aortic root (1_Yes vs 2_No)|
|raa_type |Type of ascending aorta replacement: (1_clamp vs 2_hemiarch)            |
|bsa      |Body Surface Area                                                       |
|diabetes |0_Negative vs 1_Diabetes                                                |
|hyper    |0_Negative vs 1_Hypertension                                            |
|chlstrl  |0_Negative vs 1_hyperlipidemia                                          |

* Dataset 2 ("raw_long_d")
  + contains the corresponding longitudinal measurements of these patients (298 patients)

|Variable     |Description                                                                         |
|:------------|:-----------------------------------------------------------------------------------|
|ptid         |Patient ID                                                                          |
|age          |Patient's age                                                                       |
|sex          |Patient's sex, 1_male vs 2_female                                                   |
|bav_confirmed|BAV vs TAV                                                                          |
|nc_sinus     |Replacement of the non-coronary sinus of the aortic root (1_Yes vs 2_No)            |
|raa_type     |Type of ascending aorta replacement: (1_clamp vs 2_hemiarch)                        |
|died         |Patient died, 0 = No, 1 = Yes                                                       |
|lka_d        |Date of last known alive                                                            |
|yr2death     |Year until death                                                                    |
|ao_reop      |Repeat aortic surgery (0: no reop and alive, 1: reop and alive, 2: reop and died)   |
|yr2ao_reop   |Year until re-operation                                                             |
|bsa_baseline |Body surface area at the baseline                                                   |
|dateor       |Date of operation(baseline date)                                                    |
|bsa_echo     |body surface area measured at echo dates post baseline (i.e. follow up BSA measures)|
|mdate        |Measurement date                                                                    |
|root         |Aortic root size                                                                    |
|aa           |Ascending aorta size                                                                |
|arch         |Aortic arch size                                                                    |
|src_root     |1 = ECHO, 2 = CT, 3 = MRI                                                           |
|src_aa       |1 = ECHO, 2 = CT, 3 = MRI                                                           |
|src_arch     |1 = ECHO, 2 = CT, 3 = MRI                                                           |
|day2vst      |Number of days from "dateor" to "mdate"                                             |
|yr2vst       |Number of years from "dateor" to "mdate"                                            |
|date_base    |Date of the first measurement                                                       |

* Note: for "day2vst" and "yr2vst", 0 means measured on the day of operation, negatice values indicate meaured before date of operation.  
* Outcome variables: root, ascending aorta, and arch dimension (focus on root dimension first).    
  + we can exclude the missing values. 
* Cohort variable: bicuspid aortic valve (BAV) vs Tricuspid  aortic valve (TAV). 


**Q&A**

1. For the variable "day2vst"/"yr2vst", should we exclude observations that have negative values on this variable? (i.e. should we only include measurements that took place after operation?) --> Yes.

2. Some patients had repeated aortic surgery. We need to compare the measurement date("mdate") and re-operation date, then exclude observations that measured the outcomes after re-operation.
  + check "yr2ao_reop" vs "yr2vst" (i.e. for those who did re-operation, year until measurement should be less than year until re-operation) ?  --> Yes.

3. Steve mentioned that we should pay attention to the follow-up time. For each patient, if one of the measurement took place far away from other repeated measurement, then we may need to exclude this observation.
  + How to visualize this?
  + we can calculate the time difference between measurements, but how to decide the standard of excluding observations due to large time difference?

4. Imaging data were collected from all available *echocardiography*, *CT*, and *Magnetic resonance imaging(MRI)* reports at baseline and during follow-up.
  + Sudip said these may not be relevant for our project, so we can ignore those variables for now.
  + However, Steve wanted me to discuss with you about the selection bias (which will translate into the bias within parameters). In reality, we cannot quantify selection bias. During the simulation (matching), we can simulate 2 datasets (one was measured using only ECHO, the other one was measured from 3 different sources). Then we can add some bias to the parameter to simulate the bias.
  + The question is: To what extend the bias should be introduced so that the result from the former dataset will approach the later one. If there's only a little bit study based bias, but translate to huge parameter bias, then that's the concern. But if we induce large selection bias in the study, and only observe relatively small parameter related bias, then he'll be more comfortable to use that.
--> can be discussed later

## Jan 16-20, 2023
**Meeting on Jan 17 **

* "ao_reop": Repeat aortic surgery (0: no reop and alive, 1: reop and alive, 2: reop and died)
* "dateor": baseline date
* We need to define our sample first, before doing visualization for the outcome variable (eg. histogram of year between visits, skewed?)
* Next step:
  + count the number of observations each patient has, and create a new variable "visit" and "total visit" for each patient
  + total visits vs frequency
  + create a table compare characteristics of patients with and without baseline measurement
  + other data exploration
  + Read *Quasi-Least Squares* CH2(review) and CH3
  
**Accomplishment**

* Read *Quasi-Least Squares Regression* CH2, and still reading CH3
* Exploratory Data Analysis
  * Merged baseline variables to the longitudinal dataset
  * Create variables **visit** and **total visit** to record number of measurements that each patients had.
  * Analyze baseline information for patients with longitudinal measurements (298/406)                     + Hmisc::describe() to see basic summary statistics for all variables
    + mean **age** around 64, 68% of the patients are **males**
    + **BAV** 70%(209) vs **TAV** 30%(89)
  * Summary for other variables
    + **mdate**: the earliest measurement date is in 1994
    + 3 abnormal root dimension (lowest 3 measurements), 148 missing measurements
    + **src_root**: 98% of measurements used the same method, so the measurement error should be limited
    + **yr2vst**(or day2vst): some extremely high values eg. 19 years since 1st operation date
  * Baseline measurement (**"date_base"**): the measurements that occurred before/on the day of 1st operation were considered as baseline measurement of outcome.
    + 38% patients did not have baseline measurements. 
  * Create a new variable "yr2vst_diff" to record the time difference between consecutive measurements for each patient. (But it didn't consider those without baseline measurement who had their 1st measurement a long time after their 1st operation)

**Q&A**

1.  Recall "ao_reop" (0: no reop and alive, 1: reop and alive, 2: reop and died, but there's a patient whose ao_reop = 1 but died? 
  + Re-categorize this patient to ao_reop = 2 for now.
  
2. The earliest (date_base) measurement date is 1994, but there's still 38% of the patients who did not have baseline measurements. 
  + We'll select our sample for patients who had a baseline measurement and at least 2 total visits.

3.  How can we determine the cut-off point for excluding measurements that were taken a long time after the previous measurement(for those with baseline measurements) or 1st operation(for those without baseline measurements) ?
  + First we need to select our sample.
  
  
## Jan 23-27, 2023
**Meeting on Jan 24**

* Possible questions for Steve: How to dichotomize the response variable?
* Select sample based on two conditions: 
  + 1st visit should have occurred on the date of 1st operation
  + at least 2 total visits(including the baseline measurement)
* Create a table to check the balance of baseline characteristics between BAV and TAV group
* Match the selected sample
* Re-do the plots

**Accomplishment**

* Read Quasi-Least Squares Regression Chapter3
* Selected sample based on the two conditions
* Performed pair matching base on all the baseline characteristics for the selected sample, resulting 25 pairs of patients
* Check the balance between two groups before & after matching
* Create several figures
  + distribution of root at baseline & mean root change over time by groups
  + distribution of yr2vst by groups

**Q&A**

1. The baseline characteristics are not very balance after matching (especially for the propensity score,age,sex,nc_sinus). Consider drop some covariates like nc_sinus(Replacement of the non-coronary sinus of the aortic root)?   
* --> Use only age, sex, and bsa.

2. If we look at the histogram of yr2vst, their are some patients with TAV that had yr2vst greater than 10 years. However, the visit time difference between 2 consecutive visit are all less than 10 years. So the visit time is reasonable and not too far away from the last visit, but might be a little too far away from the 1st operation for some patients. 

3. For patients with tricuspid aortic valve(TAV), they seem to had larger range of root dimension and year to visit values.
  + People with bicuspid aortic valves (BAVs) are at a higher risk of developing aortic valve disease compared to those with TAV. So if people who had TAV and still need to do the operation to replace their aortic valve, does it mean those patients had more severe condition(extremely small or large root/older/diabetes/hyper/chlstrl) and that's probability the reason why they had larger range of root dimension?

4. Since the follow up time for each patient had unequal time intervals, does it mean the AR(1) correlation structure might not capture the within-subject correlation well?  
* --> Yes. Since the data is not balance and measurements are mistimed, that's going to be a problem when we try to specify a covariance structure. A lot of structures assume balanced data, like AR(1), Toeplitz, and even the unstructured correlation structure.


## Jan 30 - Feb 03, 2023
**Meeting on Jan 31**

1. Data
* The data is not balanced, and measurements are mistimed. e.g. The time between visit 1 and visit 2 for 2 people are going to be very different.
* Next step: 
  + Simplify the data: delete observations greater than or equal to 10 visits
  + Fit a GEE model with independent correlation structure, using BAV_confirmed, visit, BAV_confirmed*visit as predictors, and root as outcome variable.

2. Exercise
* Try to solve the generalized sum of square with respect to $\alpha$.

**Accomplishment**

* Fit a GEE model with independent correlation structure.
* Derived the equation for $\hat \alpha$ from $Q(\alpha, \beta)$.
* Attend the Intro to Niagara class.

**Questions**
  
1. Why we only need one summation for i, what about j and k? 

2. After getting the derivative of $R_{ij}^{-1}(\alpha)$, how the equation 
$$\sum_{i=1}^N Z_i' \frac{\partial R_i^{-1}}{\partial \alpha} Z_i $$
is equivalent to 
$$\sum_{i=1}^N [\alpha S_1 - (1+\alpha^2) S_2] $$

3. When fitting the GEE model, I set waves = visit. The max(visit) is 9 but each patients might have different number of visit, does it matter? (family=gaussian for continuous variable?)

4. Niagara
  + Are we going to use R modules? Which R version should I load? Packages?
  + How to use it?
  + my account expires on May 10, 2023?


**No Meeting on Feb 7**


## Feb 13-17, 2023

**No meeting on Feb 14**

**Next Steps**

1. Play around with the package CWGEE that was developed as part of the paper on ordinal cluster weighted GEE. (https://github.com/ayamitani/CWGEE)
2. Run the main function
3. Go into the specific R scripts that computes each step. For example, the script names “estalpha1_ar1.R” computes the alpha estimates in AR1 from the derivation that you just went through.

## Feb 20-24, 2023

**Reading Week(No meeting)**

**Accomplishment**

1. Get familiar with the package CWGEE, tried the examples provided in the "README" file using the functions "ordCWGEE" and "ordCWGEE".
2. Understand the R script "estalpha1_ar1" based on the calculation i did last week.



## Feb 27- Mar 3, 2023

**Meeting on Feb 28**

1. Refit the GEE model, remove "scale.fix=TURE" and "bav_confirmed" main effect.
  + Result looks very similar.
  
2. Ask Steve & Sudip about how can we dichotomize the response variable, is there a good clinical cutoff?
  + An absolute value of > 4.5 cm for the aortic root or growth > 5mm would be clinically relevant endpoints (Dr. Ouzounian).
  + In our sample, 11 out of 226 observations have root > 45mm
  + Does the growth of root matter in our case? (If it does, what about a decrease > 5mm?)
  
3. Look into the derivation of alpha and the R script.
4. Generate a SSH key pair for Niagara


## Mar 6-10, 2023

**Meeting on Mar 7**

1. Create a new binary variable "root1"
  + root1 = 1 if root > 45 or root_diff > 5, otherwise root1 = 0
2. Simulation for continuous outcome (*simscript_gaussian_ps.R*)
  + generate big cohort
  + create sample data by matching patients based on propensity scores
  + repeat 1000 times and get mean coefficients and standard errors


## Mar 13-17, 2023

**Meeting on Mar 14**

1. simscript_gaussian.ps.R : 
  + add standard deviation of estimated beta
  + try N = 250, and play around with "b1" (increase from 4 to 6) so that we get 10% TAV patients
  + calculate mean bias, MSE, coverage probability

2. simscript_gaussian.matched.R:
  + add patient id
  + repeat the previous precedures

**Questions**

1. When we generate outcome values(root), we include "bav" as a main effect. But in the GEE models, we only include bav as a interaction with visit. When calculate the mean relative bias, we need to use the true beta values from the model that we used to generate outcome.
--> Don't inlcude main effect for BAV when generating root.

2. QLS can account for more than one correlation (in our case we have within-subject & within pair), but we are using GEE for now. Does it mean we're focusing on the within-subject correlation and ignoring the within-pair correlation?
--> Yes.


## Mar 20-24, 2023

**Meeting on Mar 21**

1. Comparison tables for GEE models:
  - expect to see the bias for independence is higher, since it failed to capture dependency and the consistency of estimates depends on large sample property.
  - MSE for unstructured GEE is larger because it's estimating more parameters. (18 parameters in total, including $\beta_0$, $\beta_1$, $\beta_2$, and 15 variance/covariances for 5 time points)
  - SD for $\beta_0$ is larger, and coverage probabilities are lower. (may need to investigate, but not the focus for now)
  - The mean SE in the second scenario (table 3) for AR1 and exchangeable are inflated. SE > SD, and coverage probability is higher.

2. Next steps:
  - add a random effects model with random intercept for each pair (match id), then add to the comparison table
    + the coverage probability for $\beta_0$ using linear mixed effect model is much better
  - transform the table
  

**Meeting with Sudip & Steve on Mar 24**

Notes:

1. Tentative timeline:
  + March: figure out the code for QLS
  + April: simulation for the continuous outcome
  + May: simulation for binary outcome, creat poster for Biostatistics research day
  + June & July: Write manuscript/report, finish before July 28.
2. Explain how we cleaned the data and selected the sample 
3. Two ways of simulation
  + (a) simulate hospital population and then match patients with BAV to TAV
    + there're some residual imbalance
  + (b) simulate matched pairs directly
    + covariates are already balanced

Feedback: 

1. The match cohort of the case is not necessarily representative of the original BAV or TAV cohort. (only match patients in the overlapping region for the two distribution)

2. We're only interested in $\beta_2$ (visit:BAV), and we can ignore the low coverage probability for $\beta_0$ for now. The contrast we compare is $Y_{BAV}-Y_{TAV}$.

3. Since we used random intercept to generate outcome, there's no surprise that the standard deviation of beta estimates for the exchangeable GEE is the smallest. However, the standard deviation of beta estimates for the independence GEE is almost 4 times larger (result in wide CI), which means it's inefficient (25% efficiency).

4. Make the first simulation method more realistic.
  - Now the match cohort is very representative of the original cohort($\beta_0$ estimates very close to real, bias is almost 0)
  - Two propensity score distribution: 
    + Propensity score for the matched pair cohort & propensity score for the entire cohort
    + Substantial overlap v.s. median overlap v.s. upper tail over one distribution overlap with lower tail of the other distribution
  - Is there a rule of thumb that how much overlap (in terms of proportion of the distribution or number of patients within the overlapped region) should be. 
  
5. Make the study more clinical relevant:
  + account for potential informative censoring or drop out, introduce endogeneity
  + eg. when generate outcome(root), given the root at time 2, simulate time of death since time 2. If the simulated death is before time 3, then all the subsequent observations are removed.
  
6. Submit the R markdown file that includes all my codes when submitting the report.


## Mar 27-31, 2023
**Meeting on Mar 28**

1. The sandwich estimator in GEE underestimates the true variance in small samples 
  + apply the small sample adjustment 
  + One simpel adjustment: DF (degrees of freedom)-corrected sandwich estimator proposed by MacKinnon and White (1985) in which the variance-covariance matrix is multiplied by N/(N-p) where N is the number of clusters (number of participants) and p is the number of regression parameters.

2. Simulate propensity scores for BAV and TAV groups
  + consider 3 scenarios (high/moderate/low proportion of overlapping)
  
**Accomplishment**

1. Apply DF-corrected sandwich estimator to the GEE models for matched sample.
  + Adjusted standard errors are larger, and adjusted coverage probabilities are also larger.
  
3. Modify the coefficients for baseline covariates for BAV & TAV, so that we have high/moderate/minimal overlapping propensity score distributions for the two groups and have 10% TAV.

**Questions**

1. If I simulate the propensity scores for the 2 groups separetely but with the same original baseline covariates (b1...b4), the SD and MSE for unstructured model for matched sample will be smaller. Why?
- Use this method, simulate the outcome for BAV and TAV separately with the same coefficients for high overlap scenario.

2. if i modify the values for baseline covariates, the sample size will also change, how can i make sure we got 10% TAV?

3. If we simulate the two groups with minimal overlapped propensity score distributions, then the matched sample may not have balanced baseline covariates, does it matter?
- We want high/moderate/low overlap for the cohort, but still want high overlap for the matched pairs.

4. The overlapping proportion for the high scenario is lower for the entire cohort (0.4, why?) but the sample distribution has much higher overlapping proportion.

  
## April 3-7, 2023
**Meeting on Apr 04**

- Should include BAV (the main effect) and the baseline variables(age, female, bsa) when generating the outcome
- For the cohort: fit gee models with bav*visit and the baseline varibales
- For the matched sample: root = visit + bav:visit
- Presentation: 
  + Simulation cohort(process) & 3 overlap (plots) scenarioes
  + show the relationship between bav, root, confounders(age,female,bsa)
  + only show model results(just the interaction term) for the matched sample (4 gee models)

**Accomplishment**

1. Add the 3 baseline variables & BAV for generating outcome 
2. Create presentation slides & output

## April 10-21, 2023
**Meeting on Apr 14**

- Modify the presentation slides

**Meeting on Apr 18**

- Generate high overlap propensity scores by letting the coefficient for intercept be 2.2, and let other coefficients close to 0.
- Try to generate minimal overlap distributions
- Verify the caculation for QLS with respect to $\tau$


## April 24-28, 2023

**No meeting (email)**

**Accomplishment**

1. work on the algebra with the alpha estimates for the AR1 structure
2. create poster slide for Biostatistics research day


## May 1-5, 2023
**Meeting on May 2**

- confirmed the solution for $\alpha$ and $\tau$ for the AR1 structure
  + Next step: type out the algebra using Latex and try to write the code (similar to the code in CWGEE package)
  + Start from the regular GEE with AR(1) -> initial $\beta$ and $\alpha$ estimates -> use the inital $\beta$ estimates to calculate $Z$ -> estimate $\tau$ -> estimate $\alpha$ -> estimate $\beta$, run these steps iteratively until convergence
- Next: the algebra for Exchangeable & independent(easy, don't need to estimate $\alpha$) correlation for $R_i$
- Modified poster slide 
  
**Accomplishment**

1. Submit the abstract and poster for the Biostatistics Research Day
2. work on the algebra with the alpha estimates for the exchangeable structure
3. construct the R functions for two stage alpha and tau estimates


## May 8-12, 2023
**Meeting on May 9**

QLS algorithm:
1. Use `geeglm` to get the initial beta and alpha ($\alpha_0$) estimates, use these to estimate $\tau_0$
2. Use $\alpha_0$ & $\tau_0$ to construct $F_i = Q_i \otimes R_i$, fit `geeglm` and get updated beta estimates
3. plug $\hat \beta$ into alpha and tau estimate to get the updated $\alpha_0$ and $\tau_0$
4. Run the above steps until converge, and get stage 2 estimates for alpha and tau
5. fit geeglm and get the final beta estimates


**Problems**

1.The `Z` (residuals) are the same for the geeglm models with different correlation structures, then how to get the updated alpha and tau from the beta estimates?

2. When I specify "corstr = "fixed",zcor = zcor", the alpha estimates from the model is 1.

3. For independent GEE, the bias was ever larger when I increased the sample size.


**No meeting during May 15 - May 19 **

## May 22-26, 2023
**Meeting on May 23**

- Add "bav" in the model and compare results.
- Look at the mean standardized difference for root between BAV=0 and BAV=1.
- Try bi <- rnorm(N,mean=0,sd) with sd equals 0, 0.5, 1, 2 instead of 5.
  + Expect: with sd = 0, the independent GEE should perform well.
- QLS: specify "zcor" using genZcor

**Problems**

1. How to use "genZcor"? It doesn't use any specified correlation matrix.
  + I think we should still use "fixed2Zcor" since we have a fixed working correlation matrix which is the kronecker product of the two matrices.
  
2. Initial value for alpha should not be 0.

3. Do we need to apply DF-corrected SE for qls estimates? --> Yes.

4. Independent QLS  still highly biased, and the coverage probabilities are lower than GEE for all working correlation structures.


**No meeting during May 29 - June 2**

## June 5-9, 2023
**Meeting on June 8**

1) GEE: Try large sample (N=2000) with different "sd" for random effect

2) Calculate std. difference for the cohort, if we simulate the data right, this should be greater than the std. difference of root for the matched sample
  + the difference for the cohort is smaller than the sample: this might be because the matching process results in a lot of unmatched subjects (since we set pbav0 = 0.1 and many patients with BAV=1 were unmatched)
  
3) Look at the trajectories of mean root dimension for cohort & sample

4) Try QLS with different sd & calculate mean and empirical SE for alpha and tau estimates
 (Use DF-corrected SE for QLS (also CI))
 
- Fit QLS to original data set and get alpha and tau estimates, fit GEE to get alpha estimate
 + Problem: in the original data set each patient has difference number of measurements, but i assumed balanced time points in the QLS related functions.
 
 
## June 12-16, 2023
