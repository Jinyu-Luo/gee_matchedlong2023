---
title: "Modeling Longitudinal Binary Outcomes in a Small Matched-Pair Sample with Application to Cardiovascular Data: A Simulation Study"
format:
  aft-pdf:
    keep-tex: true  
  aft-html: default
number-sections: true
author:
  - name: Jinyu Luo
    affiliations:
      - name: University of Toronto
        department: Biostatistics
        address: 155 College Street
        city: Toronto
        country: Canada
        postal-code: M5T 1P8
    orcid: 0009-0004-1101-7040
    email: jinyu.luo@mail.utoronto.ca
  - name: Chun-Po Steve Fan 
    affiliations:
      - name: University Health Network
        department: Analytic lead at Rogers Computational Program
        address: 585 University Ave
        city: Toronto
        country: Canada
        postal-code: M5G 2N2
    orcid: 0000-0002-6373-0532
    email: steve.fan@uhn.ca
  - name: Sudipta Saha 
    affiliations:
      - name: University Health Network
        department: Analytic lead at Rogers Computational Program
        address: 585 University Ave
        city: Toronto
        country: Canada
        postal-code: M5G 2N2
    email: Sudipta.Saha@uhn.ca
  - name: Aya Mitani
    affiliations:
      - name: University of Toronto
        department: Biostatistics
        address: 155 College Street
        city: Toronto
        country: Canada
        postal-code: M5T 1P8
    orcid: 0000-0002-0373-5032
    email: aya.mitani@utoronto.ca

abstract: |
  Background: This study aimed to address the challenge of modelling small sample matched-pair longitudinal data in cardiovascular research. The independent working correlation structure in Generalized Estimating Equations (GEE), a robust method widely used to model endogenous follow-up data, relies on large-sample theory. Prior research noted significant constraints due to small sample sizes for continuous outcomes. Objectives: We evaluated the validity of the working independent correlation structure in GEE, specifically focusing on binary outcomes, through a simulation study. Methods: Initially, real hospital data were fitted, assuming a working exchangeable correlation structure to estimate the true values for simulation parameters. The simulations were designed to mimic the dropout process in the real-world scenario where previous survival outcomes and associated covariates influence the longitudinal outcomes. The simulated data involves hospital cohorts with longitudinal outcomes across two exposure groups, so cohorts were matched using propensity scores based on baseline characteristics to eliminate potential confounding effects in demographic and clinical characteristics. Due to the small sample size, standard errors were adjusted by degrees of freedom to prevent underestimation by the sandwich estimator. The simulated data was then analyzed using three different correlation structures in GEE: independent, exchangeable, and autoregressive (AR1). Results: The independent working correlation structure in GEEs consistently provides the highest coverage probabilities for true parameter coefficients after adjusting for standard errors. Conclusion: Proper specification of the correlation structure is important for the robust analysis of small sample longitudinal data.
keywords: [template, demo]
reference-section-title: References
bibliography: bibliography.bib  
---




## Introduction {#sec-intro}

The generalized estimating equations (GEE) method is commonly used in longitudinal studies where the response variable for each subject is measured repeatedly over time [@LiangZeger1986]. It is an extension of the quasi-likelihood method that models the marginal expectation of the response, either discrete or binary, as a function of a set of explanatory variables [@Agresti2nd]. Instead of assuming a particular type of distribution for the outcome $Y$, each marginal mean is linked to a linear predictor and educated guess for the variance-covariance structure, which accounts for the temporal correlation among repeated measurements. Since there is no need to specify the random effects for individual subjects or clusters, GEE provides an average response in the population rather than individual-specific effects. 

Our motivation stems from the work which assessed the natural history of the aortic root in patients with bicuspid versus tricuspid aortic valves (BAVs vs. TAVs) after replacement of the aortic valve and ascending aorta at the Peter Munk Cardiac Center [@Hui2018]. The aorta is the main artery that carries blood from the heart's left ventricle to the rest of the human body. According to the 2014 ESC guidelines on diagnosing and treating aortic diseases, aorta dilatation is a clinical condition with aorta diameter greater than 40 mm, irrespective of body surface area. It is commonly present in patients with BAV, a congenital heart defect when the aortic valve has only two leaflets instead of three and affects approximately 1-2% of the general population [@Wang2021]. Patients with aortic diameter exceeding 4.5 mm are usually associated with ascending aortic events. Evidence showed that the dilation of aortic root cannot be suppressed even after AVR [@Bruce2003]. Still, other researchers found that the ascending aorta dilatation rate was similar between the BAV and TAV post-surgery [@KIM202053]. 

Given that BAV is a congenital cardiac abnormality, conducting randomized controlled trials is not feasible. Researchers often pair BAV patients with TAV patients using propensity score matching (PSM) to assess the natural history of aortic root size changes. PSM is critical in this context as it balances observed covariates between BAV and TAV groups, reducing confounding bias and enhancing the accuracy of treatment effect estimates. This technique allows for valid comparisons in observational studies, addressing selection bias and leading to more reliable conclusions about the natural history of aortic root size changes post-surgery. In practice, patients with and without exposure to interest are matched on important confounding factors such as age, sex, and calendar time and compared for the incidence of outcomes [@Iwagami2022]. In such a scenario, two distinct correlations exist: the correlation between units within the matched pair and the correlation between the temporal observations on the same patient. 
The study investigators collected participant-level demographics, health outcomes, and each participant's follow-up imaging data after the replacement of the aortic valve (AVR) and ascending aorta (RAA) from January 1900 to December 2010. This cohort consists of 406 patients, 244 of whom had follow-up measurements. Among those with follow-up visits, 172 (70.5%) patients had BAV, and the rest had TAV. Our primary outcome is whether or not the aortic root dimensions exceeded a diameter of 45 mm after the surgery. Although the data include records of patients' vitals, only the follow-up measurements of the aortic root size and baseline covariates, including age, sex, and body surface area (BSA), are included in this study. 

The first consideration in GEE analysis is the potential issue of covariate endogeneity. This concept describes the scenario when the response at time $t$ predicts the covariate value at times $s > t$ [@Diggle2002]. The issue arises because the abnormal aortic root size is associated with a higher risk of death [@KITAGAWA2013258], and the occurrence of death informs that there is no stochastic process of the deformation. The interaction effect between response and covariates is called *feedback* [@Zeger1991]. It has been shown that, based on the large-sample theory, using GEE with a working independence correlation structure can provide unbiased estimation [@Diggle2002, @LiangZeger1986]. However, the sample sizes in cardiovascular research are limited and mid-term follow-ups are incomplete due to the rarity of disease in practice. The validity of GEE with a working independence correlation structure remains unknown. The second consideration is that GEE methods within the existing R package, i.e., `geepack,` only account for the correlation between repeated measurements within one subject but ignore the correlation between matched pairs. 

This report focuses on matched longitudinal binary data with covariate endogeneity and informative dropouts. We aim to explore the validity of GEE estimates for small sample matched-pair binary outcomes and compare the estimation results with the two-stage quasi-least squares (QLS) method [@Mitani2019]. In section 2, we described the issue with the correlation structure and the construction of two-stage QLS method in more detail. The simulation study design is presented in section 3. In section 4, exploratory data analysis and model fitting results on the motivational data and simulated data are presented. Finally, we conclude this reports with a discussion in section 5. 

## Methods 

Consider a longitudinal matched data set in which subjects are grouped into pairs, and each subject contributes repeated observations of unique aortic root diameter. Let $Y_{ijk}$ be the binary outcome measurement of the $k^{th}$ visit on the $j^{th}$ subject from the $i^{th}$ pair, where $k = 1,...,t_{ij}, j = 1,2$ and $i = 1,..., N$. Here, $t_{ij}$ is the total number of visit for the $j^{th}$ subject from the $i^{th}$ pair. Let $X_{ijk}$ be the $p\times 1$ vector of covariates for the outcome $Y_{ijk}$ of the $k^{th}$ visit of the $j^{th}$ subject from the $i^{th}$ pair. In our case, $X_{ijk}$ includes BAV (exposure), *time*, and the interaction between BAV and *time*. Because different patients may have different follow-up intervals, we define *time* as the number of visits. 

Given that the outcome $Y_{ijk}$ is binary, the marginal expectation for the $k^{th}$ visit on the $j^{th}$ subject from the $i^{th}$ pair is 
\begin{equation}
\mu_{ijk} = E(Y_{ijk}|X_{ijk}). \label{eq:marginExpecatiton}
\end{equation}
We use a logit link function to relate the marginal expectation $\mu_{ijk}$ to the linear predictor. The logit link function is defined as:
$$
\begin{aligned}
\text{logit}(\mu_{ijk}) &= \log(\frac{\mu_{ijk}}{1-\mu_{ijk}})= g(\mu_{ijk}) \\
&= \beta_0 + \beta_1 \times \text{BAV}_{ij} + \beta_2 \times \text{Visit}_{ijk}+\beta_3 (\text{BAV}_{ij}\times \text{Visit}_{ijk})\\
& = X_{ij}^T\boldsymbol{\beta}
\end{aligned}
$$
where $\boldsymbol{\beta} = (\beta_0, \beta_1, \beta_2, \beta_3)^T$ is the set of regression coefficients to be estimated (Note: The exposure variable, BAV, will not change with time, so its subscript only varied for $i$ and $j$). Then, the variance can be expressed as:
\begin{equation}
\text{Var}(Y_{ijk}) = \mu_{ijk}(1-\mu_{ijk}) = h(\mu_{ijk}) \label{eq:marginVariance}
\end{equation}

### Generalized estimating equations (GEE)

Without a specific assumption about the likelihood function, generalized estimating equations (GEE) accounts the covariance structure of the repeated measures by specifying a working correlation matrix, $R(\alpha)$, which describes the correlation between repeated measures on the same subject. This paper focuses on the following three working correlations: 
$$
\underset{\text{(Independent)}}{\begin{bmatrix}
1 & 0 & \cdots & 0\\
0 & 1 & \cdots &  0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{bmatrix}}
\quad
\underset{\text{(Exchangeable)}}{\begin{bmatrix}
1 & \alpha & \cdots & \alpha\\
\alpha & 1 & \cdots & \alpha\\
\vdots & \vdots & \ddots & \vdots\\
\alpha & \alpha & \cdots & 1
\end{bmatrix}}
\quad
\underset{\text{(AR1)}}{\begin{bmatrix}
1 & \alpha & \cdots & \alpha^{t_{ij} - 1}\\
\alpha & 1 & \cdots & \alpha^{t_{ij} - 2}\\
\vdots & \vdots & \ddots & \vdots\\
\alpha^{t_{ij} - 1} & \alpha^{t_{ij} - 2} & \cdots & 1
\end{bmatrix}}
$$
. The independent working correlation assumes no correlation between repeated measures. With a correlation coefficient $\alpha$, the exchangeable working correlation assumes that the correlation between any pair of repeated measurements are constant at $\alpha$, whereas the autoregressive order one (AR1) working correlation structure assumes correlation decreases exponentially with the time lag between measures. GEE approach accounts for overdispersion or underdispersion by correcting the variance using a dispersion parameter 
$$
\text{Var}(Y_{ijk})^* = \phi\text{Var}(Y_{ijk}) = \phi h(\mu_ijk)
$$

The marginal expectation, $\mu_{ij}$ can be expressed as $\mu_{ij} = \left[g^{-1}(X_{ij1}^T \boldsymbol{\beta}), ..., g^{-1}(X_{ijt_{ij}}^T \boldsymbol{\beta})\right]^T$, then we define $A_{ij} = diag\{h(\mu_{ij1}),...h(\mu_{ijt_{ij}})\}$. The working covariance matrix for subject $j^{th}$ from the $i^{th}$ pair is $\Sigma_i = A_i^{1/2}R(\alpha)A_i^{1/2}$. Let $\phi$ be the dispersion parameter, then $\Sigma_i \phi$ is the covariance working matrix. Since our outcome is binary, the dispersion parameter $\phi$ equals to 1. 

The iterative process starts with initial guesses for the regression coefficients $\boldsymbol{\beta}^{(0)}$ and the correlation parameters $\alpha^{(0)}$. It is followed by the computation of initial marginal expectation $\mu_{ijk}^{(0)} = g^{-1}(X_{ijk}^T \boldsymbol{\beta}^{(0)})$, where $g^{-1}$ is the inverse of the link function. Then, the iterative process mainly consists of two steps: (1) update the working correlation matrix using the sample data. (2) update the regression coefficients. 

#### Update the Working Correlation Matrix

To update the working correlation matrix, we first compute the residuals, $r_{ijk}^{(m)} = Y_{ijk} - \mu_{ijk}^{(m)}$, based on the current estimates of the marginal means for each observation. Then, we estimate the correlation parameters using the residuals and construct the working correlation matrix $R_i^{(m)}(\alpha^{(m)})$ using the updated correlation parameters. 

#### Update the Regression Coefficients

For each subject, we compute the variance matrix using the current estimates of the marginal means:
\begin{equation}
\Sigma_i(\alpha) = A_i^{1/2}R(\alpha)A_i^{1/2} \label{eq:geeCovMat}
\end{equation}
where $A_i^{1/2}$ is a diagonal matrix with the variances $\mu_{ijk}^{(m)} (1-\mu_{ijk}^{(m)})$ on the diagonal. Then, the score function and the information matrix can be calculated using the current estimates [@Zeger1988]:
\begin{align}
\boldsymbol{U}(\boldsymbol{\beta}) &= \sum_{i=1}^N D_i^T \Sigma_i^{-1}(\alpha) (Y_i - \mu_i) \label{eq:geeScoreEqs}\\
I(\boldsymbol{\beta}) &= \sum_{i=1}^N D_i^T\Sigma_i^{-1}(\alpha)D_i \label{eq:geeInfoMat}
\end{align}
where $D_i = \frac{\partial\mu_i}{\partial \boldsymbol{\beta}}$. The new set of regression coefficients are obtained by solving the score function, which are then used to calculate the new marginal means. 

The final estimates of the regression coefficients $\boldsymbol{\beta}$ can be obtained by repeat the above process until convergence is achieved [@Liang1986]. This iterative process ensures that the correlation structure and the regression coefficients are appropriately updated using the sample data, resulting in consistent and efficient parameter estimates in the presence of correlated repeated measures. 

However, it has been shown that the sandwich estimator tends to underestimate standard errors (SEs) when the size sample data is small [@Mitani2019]. To overcome this issue, we can adjust the sandwich estimator by degrees of freedom [@dfcorrect]:
\begin{equation}
\Sigma_{DF} = (\frac{N}{N-p})\Sigma \label{eq:geeSigmaDF}
\end{equation}
where $N$ represents the number of patients, $p$ is the number of regression parameters.


### Quasi-least squares (QLS)

Quasi-least squares (QLS) is a two-stage approach for estimating the correlation parameters in the framework of generalized estimating equations (GEE). The method involves estimating the regression parameters and the correlation structure in two distinct stages. Proposed by @qls, the two-stage QLS method assumes that the covariance matrix are functions of the regression parameters and independent of the dispersion parameter $\phi$. Additionally, the off-diagonal elements are functions of some unknown nuisance parameters. The first stage mainly aims to estimate the regression parameters by minimizing the score function, which is consistent with the GEE approach. The difference is that the QLS method solves an unbiased estimating equation for $\alpha$, whereas the GEE method implements moment estimates of the correlation parameters [@qlspack]. The second stage refines the estimates of the correlation parameters based on the residuals from the first stage and updates the working correlation matrix. By iterating between these two stages, the two-stage QLS approach ensures robust and efficient estimates of the regression parameters while appropriately accounting for the correlation within the data.

In our context, the goal is to fit a logistic regression model to describe the relationship between the covariates and the binary outcome of whether the aortic root diameter exceeds a specified threshold: 
\begin{equation}
\text{logit}[\text{Pr}(Y_{ijk} = 1)] = X_{ijk}^T \boldsymbol{\beta} = \mu_{ijk} \label{eq:qlsMarginExpectation}
\end{equation}
where $Y_{ijk}$ is the binary outcome for the $k^{th}$ visit of the $j^{th}$ subject from the $i^{th}$ pair, $X_{ijk}$ is the vector of covariates, and $\boldsymbol{\beta}$ is the vector of regression coefficients. The dependence between the binary response will be incorporated in the working correlation matrix.

Using matrix notation, we group each of the response vectors of the $j^{th}$ unit from the $i^{th}$ pair as $\boldsymbol{Y}^T_{ij} = (\boldsymbol{Y}^T_{ij1}, ..., \boldsymbol{Y}^T_{ijt_{ij}})$. Similarly, let $\boldsymbol{u}_{ij} = E(\boldsymbol{Y}_{ij})$, where $\boldsymbol{u}^T_{ij} = (\boldsymbol{u}^T_{ij1}, ..., \boldsymbol{u}^T_{ijt_{ij}})$ and $\boldsymbol{u}^T_{ijk} = (\boldsymbol{u}^T_{ijk0}, \boldsymbol{u}^T_{ijk1})$ representing the outcome for TAV and BAV, respectively. For each pair $i$, define the complete data matrix for the $j^{th}$ subject as 
$$
X_{ij} = (\boldsymbol{1}_{t_{ij}} \otimes 1, X_{0ij} \otimes 1)
$$
where $X_{0ij}$ indicates that this matrix contains the baseline covariates for the $j^{th}$ subject in the $i^{th}$ pair and $\otimes$ represents the Kronecker product [@parson2006]. Define $A_{ij}$ and $\Sigma_{ij}$ in the same way as the GEE approach, and let $\boldsymbol{Z}_{ij}^T = (\boldsymbol{Y}_{ij} - \mu_{ij})^T A_{ij}^{1/2}$. Then, the generalized sum of squares is defined as [@Mitani2019]:
\begin{equation}
Q_W(\alpha, \beta) = \sum_{i=1}^N\frac{1}{2}\left(\frac{1}{t_{i1}}Z^T_{i1}\Sigma_{i1}^{-1} Z_{i1} + \frac{1}{t_{i2}}Z^T_{i2}\Sigma_{i2}^{-1} Z_{i2}\right) = 0 \label{eq:qlsGSS}
\end{equation}

#### Specification of $\Sigma_{ij}$

The working correlation is specified using the approach proposed by @parson2006. Let $\Sigma_i(\Gamma)$ be the working correlation matrix of the outcome vector for the $i^{th}$ pair with the working correlation parameter $\Gamma^T = (\tau^T, \alpha^T)$ where $\tau$ represents the correlation within matched pairs and $\alpha$ denotes the correlation between longitudinal measurements. The working correlation matrix can be specified as 
\begin{equation}
\Sigma_i = C_i(\tau) \otimes R_i(\alpha) \label{eq:qlsCovMat}
\end{equation}
where 
$$
C_i(\tau) = 
\begin{bmatrix}
1 & \tau \\
\tau & 1
\end{bmatrix}; 
\quad 
R_i(\alpha) = 
\begin{bmatrix}
1 & \alpha & \alpha^2 & \alpha^3\\
\alpha & 1 & \alpha & \alpha^2\\
\alpha^2 & \alpha & 1 & \alpha\\
\alpha^3 & \alpha^2 & \alpha & 1
\end{bmatrix}
$$
assuming the total number of visit $t_{ij} = 4$ and the working correlation structure is AR1. 

#### Estimation of $\beta$ and $\alpha$

Taking the partial derivative of $Q_W(\alpha, \boldsymbol{\beta})$ \eqref{eq:qlsGSS} with respect to $\boldsymbol{\beta}$ and setting it equal to 0, the score function is 
\begin{equation}
S(\boldsymbol{\beta}) = \sum_{i=1}^N \frac{1}{2} \left(\frac{1}{t_{i1}}\boldsymbol{D}_{i1}^T \boldsymbol{W}_{i1}^{-1}(\boldsymbol{Y}_{i1} - \boldsymbol{\mu}_{i1}) + \frac{1}{t_{i2}}\boldsymbol{D}_{i2}^T \boldsymbol{W}_{i2}^{-1}(\boldsymbol{Y}_{i2} - \boldsymbol{\mu}_{i2})\right) = 0  \label{eq:qlsScore}
\end{equation}
where $\boldsymbol{D}_{ij} = \partial \boldsymbol{\mu}_{ij} / \partial \beta$ and $\boldsymbol{W}_{ij} = A_{ij}^{1/2} Sigma_{ij}(\alpha) A_{ij}^{1/2}$. With the correct specification of the working correlation $Sigma_{ij}$, we can interpret the marginalization as the binary outcomes of aortic root diameter measurements over time for each subject within a matched pair. This approach allows us to model the effects of covariates while accounting for within-pair correlations and longitudinal dependencies.

The estimation of $\alpha$ involves two stages. The first stage estimates $\alpha$ by taking the partial derivative of $Q_W(\alpha, \beta)$ with respect to $\alpha$ and setting it equal to 0:
\begin{equation}
\sum_{i=1}^N \frac{1}{2} \left(\frac{1}{t_{i1}}\boldsymbol{Z}_{i1}^T\frac{\partial \Sigma_{i1}^{-1}}{\partial \alpha}\boldsymbol{Z}_{i1} + \frac{1}{t_{i2}}\boldsymbol{Z}_{i2}^T \frac{\partial \Sigma_{i2}^{-1}}{\partial \alpha}\boldsymbol{Z}_{i2}\right) = 0  \label{eq:stage1alpha}
\end{equation}
Since the stage one estimator for $\alpha$ is asymptotically biased [@CHAGANTY1999145], a consistent estimator can be obtained through the second stage by solving the following equation for $\alpha$: 
\begin{equation}
\sum_{i=1}^N \frac{1}{2} \left[\frac{1}{t_{i1}}\text{trace}\left(\frac{\partial \Sigma_{i1}^{-1} (\hat{\alpha}_0)}{\partial \hat{\alpha}_0}\Sigma_{i1}(\alpha)+\frac{\partial \Sigma_{i2}^{-1} (\hat{\alpha}_0)}{\partial \hat{\alpha}_0}\Sigma_{i2}(\alpha)\right)\right] \label{eq:stage2alpha}
\end{equation}
where $\hat{\alpha}_0$ is the solution to equation \eqref{eq:stage1alpha}. The estimator for $\boldsymbol{\beta}$ and $\alpha$ are obtained by choosing a starting value for $\boldsymbol{\beta}$ from fitting a GLM assuming independence between observations and iterating through equations \eqref{eq:qlsScore}, \eqref{eq:stage1alpha}, and \eqref{eq:stage2alpha}. 

### AR1 working correlation structure 

Given that all the subjects from the motivational study underwent the aortic root replacement surgery, it is reasonable to believe that the correlation of longitudinal measurements is decreasing as time passes. Therefore, AR1 is assumed to be the true working correlation structure. Then, a closed-form solution for equation \eqref{eq:stage1alpha} can be obtained by solving the following formula:
\begin{equation}
\hat{\alpha}_0 = \frac{F_a + \sqrt{(F_1+F_b)(F_a-F_b)}}{F_b} \label{eq:ar1alpha1}
\end{equation}
where 
$$
F_a = \sum_{i=1}^N \frac{1}{2} \sum_{j=1}^2 \frac{1}{t_{ij}}\left[\sum_{k=1}^{t_{ij}} Z_{ijk}^T C_i^{-1} Z_{ijk} + \sum_{k=1}^{t_{ij}-1}Z_{ijk}^T C_i^{-1} Z_{ijk}\right]
$$
and 
$F_b = 2\sum_{i=1}^N \frac{1}{2}\sum_{j=1}^2\frac{1}{t_{ij}}\sum_{k=1}^{t_{ij}-1} Z_{ijk}^T C_i^{-1} Z_{ijk+1}$. The stage two estimator for $\alpha$ can be obtained by solving the equation \eqref{eq:stage1alpha} [@Mitani2019]:
\begin{equation}
\hat{\alpha} = \frac{2\hat{\alpha}_0}{1+\hat{\alpha}_0^2} \label{eq:ar1alpha2}
\end{equation}. Detailes of the derivations for equation \eqref{eq:ar1alpha1} and \eqref{eq:ar1alpha2} are shown in the Appendix. 

### Modeling Motivational Data 

To model the motivational data, we first selected all patients who had at least one follow-up visit with baseline measurement being taken at the day of operation. Then, a binary outcome was created by setting it to 1 if the current root size measurement is over 45 mm or the growth from the previous measure is over 5 mm, and 0 otherwise. A maximal of 6 records (including the baseline) for each patient were kept for further analysis. Then, assuming that dropouts follows Weibull distribution, 5 survival models were fitted from visit 2 to 6. The fitting coefficients were extracted for simulation. Next, a logistic regression model is fitted on the data which included the baseline measurements so that we can pair BAV patients with TAV patients using the `pairmatch` function from the R package `optmatch` [@optmatch]. Then, we applied the function `geeglm` from the R package `geepack` on the matched data set to produce the estimation results using independence, exchangeable, and AR1 working correlation structures. Finally, customized functions for implementing QLS are applied on the matched data. 

## Simulation Study 

### Full Data Simulation 




```{r, include=FALSE}
load("~/Documents/Projects/Practicum_GEE/data/realcoefs.RData")
n_patients <- 250 
n_sim <- 1000
maxT <- 6
rho <- 0.3
alpha_ci <- 0.05

gammas <- round(ps_coefs, 3)
true_coefs <- round(red_ar1$coefficients$Estimate, 3)
```





The simulation process for generating one set of cohort data involves several steps to model both covariates and binary outcomes for each patient. For each patient, we first simulated baseline covariates, including age, sex, and body surface area (BSA), by assuming a normal distribution for continuous data and a binomial distribution for binary data. To calculate the probability of having BAV, we used a logistic regression model of the form:
$$
\text{Pr}(\text{BAV} = 1 | \text{Age, Sex, BSA}) = \frac{\exp(\gamma_0 + \gamma_1 \cdot \text{Age} + \gamma_2 \cdot \text{Sex} + \gamma_3 \text{BSA})}{1+\exp(\gamma_0 + \gamma_1 \cdot \text{Age} + \gamma_2 \cdot \text{Sex} + \gamma_3 \text{BSA})}
$$
where $\gamma_0 = `r gammas[1]`, \gamma_1 = `r gammas[2]`, \gamma_2 = `r gammas[3]`$, and $\gamma_3 = `r gammas[4]`$. This probability is then used to generate the exposure variable, BAV, under binomial distribution. To simulate longitudinal matched data with binary outcomes, the marginal probability of having positive outcome is obtained by using another logistic regression model that includes BAV, visit times, and their interaction： 
$$
\text{Pr}(Y = 1 | \text{BAV, Visit, BAV} \times \text{Visit}) = \frac{\exp(\beta_0 + \beta_1 \cdot \text{BAV} + \beta_2 \cdot \text{Visit} + \beta_3 \cdot \text{BAV}\cdot \text{Visit})}{1+\exp(\beta_0 + \beta_1 \cdot \text{BAV} + \beta_2 \cdot \text{Visit} + \beta_3 \cdot \text{BAV}\cdot \text{Visit})}.
$$
where $\beta_0 = `r true_coefs[1]`, \beta_1 = `r true_coefs[2]`, \beta_2 = `r true_coefs[3]`$ and $\beta_3 = `r true_coefs[4]`$. 

Each subject is assumed to have six visits, including the baseline measurement, so six marginal probabilities are produced through this process. The true correlation among longitudinal measurements is set to be 0.3, and the working covariance is assumed to follow a first-order autoregressive structure, where correlations decrease with the distance between observations. Finally, the binary longitudinal outcomes are generated using these marginal probabilities with the `cBern` function within the `CorBin` package in R. This package, developed by @cbernWei, simulates binary outcomes by ensuring a positive definite correlation matrix and restricting the range of correlation coefficients using Prentice constraints [@Prentice1988]. 

### Informative Dropout Simulation and Propensity Score Matching

To simulate the informative dropouts, we first modeled the dropout pattern by fitting the motivational data using survival models at every visit except the baseline and the last measurement. For each visit, the status indicator was set to 1 if the maximum number of visits for the subject was the current visit, indicating dropout from the study with no further follow-up measurements. For example, the first survival model was fitted at visit 2 since visit 1 is the baseline measurement, and subjects with a total of 2 visits were assigned a status of 1. The event time was defined as the total number of visits. Given that the total number of visits was 6, four survival models were fitted to the real data. The fitting coefficients, including scale and shape parameters, were extracted from these models. These parameters were then used in the `simsurv` function to simulate dropouts at each follow-up visit for the simulated data [@simsurv20]. Once the dropout process is completed, propensity scores are calculated based on the logistic regression with the three baseline covariates, which are then been applied with the `pairmatch` function in the `optmatch` R package to match BAV subjects with TAV subjects. Then, we applied QLS with independence, AR1, and exchangeable working correlation structures to estimate the regression coefficients. We also applied GEE functions from existing R package `geepack` with the three working correlation structures [@Hojsgaard2006]. 

We simulated 1,000 data sets, each containing 250 subjects with up to 6 observations per subject, using the described simulation process. For each simulation and method, we computed the mean estimates, the mean standard errors, mean robust standard errors (MSEs), the standard deviations (SD), mean bias, and mean relative bias for each regression coefficient estimate. The mean bias was obtained by calculating the difference between the mean estimates and the respective true value, which was then divided by the true value to obtain the mean relative bias. The coverage probability was determined by calculating the proportion of the 95% confidence intervals that included the respective true parameter values among the 1,000 fitting results. 


## Results 


### Analysis of Motivational Data 

Table 1 compares the estimates, standard errors (SE), and SE adjusted for degrees of freedom (SE-DF) for the GEE and QLS methods across different working correlation structures in modeling the binary outcome of aortic root diameter in BAV patients using data from the Peter Munk Cardiac Center. Within each method, parameter estimates are stable across correlation structures, showing minor variations. The GEE method estimates for $\beta_1$ (BAV) range from -1.093 to -1.117, while QLS estimates range from -0.080 to -0.091. For $\beta_2$ (Visit), GEE estimates range from -0.052 to -0.063, and QLS estimates range from 0.007 to 0.012. The interaction term $\beta_4$ shows a positive effect across both methods, with GEE estimates ranging from 0.192 to 0.224 and QLS estimates ranging from 0.009 to 0.016. The correlation parameter $\alpha$ are all around 1 regardless of methods and correlation structures. Notably, the estimation for $\tau$ is around zero in QLS, indicating minimal within-pair correlation.

Across methods, there are notable differences in parameter estimates within the same correlation structure. GEE consistently shows stronger associations for BAV status and its interaction with time compared to QLS, which yields more conservative estimates. The GEE method suggests a stronger negative association between BAV status and the binary outcome and a positive interaction effect between visit and BAV status, while the QLS method indicates smaller effects. 




```{r, include=FALSE}
library(tidyverse)
library(kableExtra)
library(ggpubr)
load("~/Documents/Projects/Practicum_GEE/Outputs/fits_analysis.RData")
load("~/Documents/Projects/Practicum_GEE/data/se_df.RData")
source("~/Documents/Projects/Practicum_GEE/fits_analysis.R")
```

```{r motivationFit}
parameters <- c("$\\beta_0$ (Intercept)", "$\\beta_1$ (BAV)", "$\\beta_2$ (Visit)", "$\\beta_4$ (Visit $\\times$ BAV", "$\\alpha$")
ind_df <- data.frame(est = c(coef(red_ind)$Estimate, NA, qls_ind_red$coefficients, NA, qls_ind_red$tau),
                     se = c(coef(red_ind)$Std.err, NA, qls_ind_red$se, NA, NA), 
                     se_df = c(df_se[1:4, 3], NA, df_se[13:16, 3], NA, NA))
ar1_df <- data.frame(est = c(coef(red_ar1)$Estimate, red_ar1$geese$correlation$estimate,
                             qls_ar1_red$coefficients, qls_ar1_red$alpha, qls_ar1_red$tau),
                     se = c(coef(red_ar1)$Std.err, red_ar1$geese$correlation$san.se,
                            qls_ar1_red$se, NA, NA), 
                     se_df = c(df_se[5:8, 3], NA, df_se[17:20, 3], NA, NA))
exch_df <- data.frame(est = c(coef(red_ex)$Estimate, red_ex$geese$correlation$estimate, 
                              qls_exch_red$coefficients, qls_exch_red$alpha,
                              qls_exch_red$tau),
                     se = c(coef(red_ex)$Std.err, red_ex$geese$correlation$san.se,
                            qls_exch_red$se, NA, NA), 
                     se_df = c(df_se[9:12, 3], NA, df_se[21:24, 3], NA, NA))
options(knitr.kable.NA = "")
data.frame(method = c(rep("GEE", 5), rep("QLS", 6)), 
           parameter = c(rep(parameters, 2), "$\\tau$")) %>% 
  cbind(ind_df) %>% 
  cbind(ar1_df) %>% 
  cbind(exch_df) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 3, escape = F,
      col.names = c("Method", "Parameter", "Estimate", "SE", "SE-DF", 
                    "Estimate", "SE", "SE-DF", "Estimate", "SE", "SE-DF"), 
      caption = "\\footnotesize Comparison of Estimations from GEE and QLS using longitudinal data from the Peter Munk Cardiac Center.") %>%
  kable_styling(latex_options = c("hold_position", "scale_down"), 
                font_size = 6) %>% 
  kable_paper("striped", full_width = F) %>% 
   column_spec(1, bold = T) %>%
  collapse_rows(columns = 1, valign = "top") %>% 
  add_header_above(c(" ", " ","Independence" = 3, "AR1" = 3, "Exchangeable" = 3)) 
```





### Simulation Results

All 1000 simulations converged for both GEE and QLS fits. Each simulation was checked for extreme values. Seventy-one simulations showed extremely large standard errors when fitted with GEE, while only one simulation (the 477th) exhibited an extreme standard error using the QLS approach. These problematic simulations were removed from further analysis.

Table 2 presents the true coefficient coverage probabilities for GEE and QLS fits. Surprisingly, the coverage probability for the intercept, visit, and the interaction effect between BAV and visit under the QLS framework is near zero, while the coverage probabilities using GEE achieved at least 80%, except for the visit effect.




```{r, results='hide', include=FALSE}
gee_conv_info
qls_conv_info
```

```{r}
coverage_gee_adj %>% cbind(coverage_gee_unadj %>% select(-term)) %>% 
  cbind(coverage_qls_adj %>% select(-term)) %>% 
  cbind(coverage_qls_unadj %>% select(-term)) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 3, escape = F, 
        caption = "\\footnotesize The Coverage Probability of Regression Coefficient Estimation from GEE and QLS.", 
        col.names = c("Term", rep(c("Empirical SE", "DF-SE"),4))) %>% 
  kable_styling(latex_options = c("hold_position", "scale_down"), font_size = 8) %>% 
  kable_paper("striped", full_width = F) %>% 
  add_header_above(c(" " = 1, "GEE" = 4, "QLS" = 4))
```





Figure 1 compares the mean estimates of the GEE and QLS methods across three correlation structures (Independence, AR1, Exchangeable) from 1000 simulations without adjusting for age, sex, and BSA. The mean estimation result from models that were adjusted for aforementioned confounding covariates are very similar to results in figure 1, so it won't display in the report. Adjusted and unadjusted denote whether the 95% confidence interval has been adjusted by degrees of freedom, calculated as the number of subjects divided by the difference between the number of subjects and the number of parameter coefficients. The black dashed line represents the true coefficients for each term. Generally, the correction of the sandwich estimator does not influence the estimation results, as there is no significant difference between the adjusted and unadjusted plots. Both methods failed to capture the feature that the number of visits is negatively associated with a positive outcome.

The first row presents the mean estimation results using the GEE approach. The mean estimate for the interaction effect (BAV × Visit) is almost the same as the true value, while the 95% confidence intervals fail to capture the true value of the number of visits. Although the true coefficient for BAV is higher than the mean estimate, it is captured within the 95% confidence interval. The second row presents the mean estimation results using the QLS approach. All the confidence intervals are much smaller than those from the GEE method. The QLS approach fails to capture the true regression coefficient for the interaction effect and the number of visits, but the mean estimate of BAV is very close to the true value.




```{r, fig.cap="Comparison of Estimation Results From 1000 Simulation"}
red_summary %>% 
  left_join(true_set %>% 
              mutate(term = case_when(term == "visit" ~ "Visit", 
                                      term == "bav" ~ "BAV", 
                                      term == "bav:visit" ~ "BAV:Visit")), 
            by = "term") %>% 
  rename(true_value = coefficient) %>% 
  # filter(adjusted == "Unadjusted") %>% 
  ggplot(aes(x = model, y = mean_estimate, color = model)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2, position = position_dodge(width = 0.5)) +
  geom_hline(aes(yintercept = true_value), linetype = "dashed", color = "black") +
  facet_grid(type ~ adjusted+term, scales = "free_y") +
  labs(title = "Without adjustment by age, sex, and BSA", 
       x = "Model",
       y = "Mean Estimate",
       color = "Model") +
  theme_classic() +
  theme(panel.grid.major = element_line(color = "grey80"),
        panel.grid.minor = element_line(color = "grey90"), 
        panel.grid.major.x = element_blank(), 
        axis.text.x = element_blank(), 
        axis.ticks.x = element_blank()) 
```




Table 3 compares the mean estimation of the correlation among longitudinal measurements using GEE and QLS methods, both adjusted and unadjusted for age, sex, and BSA. Without adjusting for confounding covariates, the mean estimate of the correlation using GEE approach with AR1 working correlation is 0.196 with a relative bias of -0.345, while QLS provides a closer estimate (0.247) with a smaller relative bias of -0.176. For the exchangeable correlation, GEE's mean estimate is highly biased, while QLS remains robust. Adjusting for confounders improves the estimates for both methods under AR1, with QLS showing more accurate and less biased estimates with lower MSE. However, when the working correlation is misspecified as exchangeable, both methods yield biased estimates.




```{r}
rho_df <- data.frame(adjusted = c(rep("Unadjusted", 4), rep("Adjusted", 4)), 
                     corstr = c(rep("AR1", 2), rep("Exchangeable", 2)), 
                     model = rep(c("GEE", "QLS"), 2),rho = rho) %>% 
  cbind(calculate_mean_rho(gee_sim_results$ar1_mdl_full) %>% 
          rbind(calculate_mean_rho(qls_sim_results$ar1_mdl_full)) %>%
          rbind(calculate_mean_rho(gee_sim_results$exch_mdl_full)) %>% 
          rbind(calculate_mean_rho(qls_sim_results$ar1_mdl_full)) %>% 
          rbind(calculate_mean_rho(gee_sim_results$ar1_mdl_red)) %>% 
          rbind(calculate_mean_rho(qls_sim_results$ar1_mdl_red)) %>% 
          rbind(calculate_mean_rho(gee_sim_results$exch_mdl_red)) %>% 
          rbind(calculate_mean_rho(qls_sim_results$exch_mdl_red)))

colnames(rho_df) <- c("", "Correlation", "Marginal Model", "True Value", 
                      "rho Estimate", "SD", "Bias", "Relative Bias", "MSE")

rho_df %>%
  kable(format = "latex", booktabs = TRUE, digits = 3, escape = F, 
        caption = "\\footnotesize Comparison of mean estimation for the correlation among longitudinal measurements using GEE and QLS methods, with and without adjustment for age, sex, and BSA.") %>% 
  kable_styling(latex_options = c("hold_position", "scale_down"), 
                font_size = 8) %>% 
  kable_paper("striped", full_width = F) %>% 
  column_spec(1, bold = T) %>%
  collapse_rows(columns = 1:2, valign = "top") %>% 
  footnote(symbol = "Adjustments were made during the model fitting by include or exclude age, sex, and BSA",
           footnote_as_chunk = T)
```

```{r}
unajd_df <- data.frame(
  corstr = c(rep("Independence", 8), rep("AR1", 8), rep("Exchangeable", 8)), 
  type = rep(c(rep("GEE", 4), rep("QLS", 4)), 3)) %>% 
  cbind(cbind(evaluate_fits(gee_sim_results$ind_mdl_red, true_set) %>% 
                rbind(evaluate_fits(qls_sim_results$ind_mdl_red, true_set)) %>% 
                rbind(evaluate_fits(gee_sim_results$ar1_mdl_red, true_set)) %>% 
                rbind(evaluate_fits(qls_sim_results$ar1_mdl_red, true_set)) %>% 
                rbind(evaluate_fits(gee_sim_results$exch_mdl_red, true_set)) %>% 
                rbind(evaluate_fits(qls_sim_results$exch_mdl_red, true_set)) %>% 
                select(-c(bias, mse)))) %>% 
  relocate(coefficient, .after = term) %>% 
  mutate(term = case_when(term == "(Intercept)" ~ "Intercept", 
                          term == "bav" ~ "BAV", 
                          term == "bav:visit" ~ "BAV:Visit", 
                          term == "visit" ~ "Visit")) 

names(unajd_df)[6] <- paste0(names(unajd_df)[6], footnote_marker_symbol(1))
kable(unajd_df, format = "latex", booktabs = TRUE, digits = 3, escape = F,
      caption = "//footnotesize Simulation results from models without adjustment for confounding covariates",
      col.names = c("Correlation", "Type", "Term", "True Value", "Mean Estimate", 
                    "SD(Estimate)", "Mean SD", "Mean DF-SE", "Relative Bias")) %>%
  kable_styling(latex_options = c("hold_position", "scale_down"), 
                font_size = 8) %>% 
  kable_paper("striped", full_width = F) %>% 
   column_spec(1, bold = T) %>%
  collapse_rows(columns = 1:2, valign = "top") %>% 
  footnote(symbol = "Standard Deviation",
           symbol_title = "SD ",
           footnote_as_chunk = T)
```




Table 4 displays the mean estimation for model fits with adjustment for confounding variables. 



```{r}
adj_df <- data.frame(
  corstr = c(rep("Independence", 8), rep("AR1", 8), rep("Exchangeable", 8)), 
  type = rep(c(rep("GEE", 4), rep("QLS", 4)), 3)) %>% 
  cbind(cbind(evaluate_fits(gee_sim_results$ind_mdl_full, true_set) %>% 
                rbind(evaluate_fits(qls_sim_results$ind_mdl_full, true_set)) %>% 
                rbind(evaluate_fits(gee_sim_results$ar1_mdl_full, true_set)) %>% 
                rbind(evaluate_fits(qls_sim_results$ar1_mdl_full, true_set)) %>% 
                rbind(evaluate_fits(gee_sim_results$exch_mdl_full, true_set)) %>% 
                rbind(evaluate_fits(qls_sim_results$exch_mdl_full, true_set)) %>% 
                filter(term %in% c("(Intercept)", "bav", "bav:visit", "visit")))) %>% 
  relocate(coefficient, .after = term) %>% 
  mutate(term = case_when(term == "(Intercept)" ~ "Intercept", 
                          term == "bav" ~ "BAV", 
                          term == "bav:visit" ~ "BAV:Visit", 
                          term == "visit" ~ "Visit")) 

names(adj_df)[6] <- paste0(names(adj_df)[6], footnote_marker_symbol(1))
kable(adj_df, format = "latex", booktabs = TRUE, digits = 3, escape = F,
      caption = "//footnotesize Simulation results from models adjusted for confounding covariates",
      col.names = c("Correlation", "Type", "Term", "True Value", "Mean Estimate", 
                    "SD(Estimate)", "Mean SD", "Relative Bias")) %>%
  kable_styling(latex_options = c("hold_position", "scale_down"), 
                font_size = 8) %>% 
  kable_paper("striped", full_width = F) %>% 
   column_spec(1, bold = T) %>%
  collapse_rows(columns = 1:2, valign = "top") %>% 
  footnote(symbol = "Standard Deviation and Mean Squared Error",
           symbol_title = "SD and MSE: ",
           footnote_as_chunk = T)
```




## Discusssion


### Code chunk {#sec-chunks}





```{r, fig.width=10, fig.height=8, include=FALSE, results='hide', eval=FALSE}
ggarrange(red_mdl_plots$BAV+labs(title = "BAV"), 
          red_mdl_plots$Visit+labs(title = "Visit"), 
          red_mdl_plots$`BAV:Visit`+labs(title = "BAV:Visit"), 
          labels = c("","",""), 
          ncol = 1, nrow = 3, heights = c(1, 1, 1),
          common.legend = TRUE, legend = "bottom")

ggarrange(full_mdl_plots$BAV+labs(title = "BAV"), 
          full_mdl_plots$Visit+labs(title = "Visit"), 
          full_mdl_plots$`BAV:Visit`+labs(title = "BAV:Visit"), 
          labels = c("","",""), 
          ncol = 1, nrow = 3, heights = c(1, 1, 1),
          common.legend = TRUE, legend = "bottom")

ggarrange(full_mdl_plots$Age+labs(title="Age"),
          full_mdl_plots$BSA+labs(title="BSA"),
          full_mdl_plots$Male+labs(title="Gender (male)"),labels = c("","",""), 
          ncol = 1, nrow = 3, heights = c(1, 1, 1),
          common.legend = TRUE, legend = "bottom")
```

```{r, eval=FALSE}
adj_ind <- data.frame(corstr = rep("Independence", 14),
           type = c(rep("GEE", 7), rep("QLS", 7))) %>% 
  cbind(evaluate_fits(gee_sim_results$ind_mdl_full, true_set) %>% 
           rbind(evaluate_fits(qls_sim_results$ind_mdl_full, true_set))) %>% 
  relocate(coefficient, .after = term) %>% 
  mutate(term = case_when(term == "(Intercept)" ~ "Intercept", 
                          term == "bav" ~ "BAV", 
                          term == "bav:visit" ~ "BAV:Visit", 
                          term == "visit" ~ "Visit", 
                          term == "age" ~ "Age", 
                          term == "bsa" ~ "BSA", 
                          term == "male" ~ "Male")) 

names(adj_ind)[1] <- paste0(names(adj_ind)[1], footnote_marker_symbol(1))
names(adj_ind)[6] <- paste0(names(adj_ind)[6], footnote_marker_symbol(1))
names(adj_ind)[9] <- paste0(names(adj_ind)[9], footnote_marker_symbol(1))

kable(adj_ind, format = "latex", booktabs = TRUE, digits = 3, escape = F,
      col.names = c("Corstr", "Type", "Term", "True Value", "Mean Estimate", 
                    "SD", "Bias", "Relative Bias", "MSE")) %>%
  kable_styling(latex_options = c("hold_position", "scale_down"), 
                font_size = 8) %>% 
  kable_paper("striped", full_width = F) %>% 
   column_spec(1, bold = T) %>%
  collapse_rows(columns = 1:2, valign = "top") %>% 
  footnote(symbol = "Correlation Structure, Standard Deviation and Mean Squared Error",
           symbol_title = "Corstr, SD, MSE: ",
           footnote_as_chunk = T)
```

```{r, eval=FALSE}
adj_ar1 <- data.frame(corstr = rep("AR1", 14),
           type = c(rep("GEE", 7), rep("QLS", 7))) %>% 
  cbind(evaluate_fits(gee_sim_results$ar1_mdl_full, true_set) %>% 
           rbind(evaluate_fits(qls_sim_results$ar1_mdl_full, true_set))) %>% 
  relocate(coefficient, .after = term) %>% 
  mutate(term = case_when(term == "(Intercept)" ~ "Intercept", 
                          term == "bav" ~ "BAV", 
                          term == "bav:visit" ~ "BAV:Visit", 
                          term == "visit" ~ "Visit", 
                          term == "age" ~ "Age", 
                          term == "bsa" ~ "BSA", 
                          term == "male" ~ "Male")) 

names(adj_ar1)[1] <- paste0(names(adj_ar1)[1], footnote_marker_symbol(1))
names(adj_ar1)[6] <- paste0(names(adj_ar1)[6], footnote_marker_symbol(1))
names(adj_ar1)[9] <- paste0(names(adj_ar1)[9], footnote_marker_symbol(1))

kable(adj_ar1, format = "latex", booktabs = TRUE, digits = 3, escape = F,
      col.names = c("Corstr", "Type", "Term", "True Value", "Mean Estimate", 
                    "SD", "Bias", "Relative Bias", "MSE")) %>%
  kable_styling(latex_options = c("hold_position", "scale_down"), 
                font_size = 8) %>% 
  kable_paper("striped", full_width = F) %>% 
   column_spec(1, bold = T) %>%
  collapse_rows(columns = 1:2, valign = "top") %>% 
  footnote(symbol = "Correlation Structure, Standard Deviation and Mean Squared Error",
           symbol_title = "Corstr, SD, MSE: ",
           footnote_as_chunk = T)
```

```{r,eval=FALSE}
adj_exch <- data.frame(corstr = rep("Exchangeable", 14),
           type = c(rep("GEE", 7), rep("QLS", 7))) %>% 
  cbind(evaluate_fits(gee_sim_results$exch_mdl_full, true_set) %>% 
           rbind(evaluate_fits(qls_sim_results$exch_mdl_full, true_set))) %>% 
  relocate(coefficient, .after = term) %>% 
  mutate(term = case_when(term == "(Intercept)" ~ "Intercept", 
                          term == "bav" ~ "BAV", 
                          term == "bav:visit" ~ "BAV:Visit", 
                          term == "visit" ~ "Visit", 
                          term == "age" ~ "Age", 
                          term == "bsa" ~ "BSA", 
                          term == "male" ~ "Male")) 

names(adj_exch)[1] <- paste0(names(adj_exch)[1], footnote_marker_symbol(1))
names(adj_exch)[6] <- paste0(names(adj_exch)[6], footnote_marker_symbol(1))
names(adj_exch)[9] <- paste0(names(adj_exch)[9], footnote_marker_symbol(1))

kable(adj_exch, format = "latex", booktabs = TRUE, digits = 3, escape = F,
      col.names = c("Corstr", "Type", "Term", "True Value", "Mean Estimate", 
                    "SD", "Bias", "Relative Bias", "MSE")) %>%
  kable_styling(latex_options = c("hold_position", "scale_down"), 
                font_size = 8) %>% 
  kable_paper("striped", full_width = F) %>% 
   column_spec(1, bold = T) %>%
  collapse_rows(columns = 1:2, valign = "top") %>% 
  footnote(symbol = "Correlation Structure, Standard Deviation and Mean Squared Error",
           symbol_title = "Corstr, SD, MSE: ",
           footnote_as_chunk = T)
```







But you can set `echo` option to `true` locally in the chunk


### Text color {#sec-summary}

Our format makes applying color on inline text possible using the `[content]{color=<name>}` syntax. Let's see an example.

Here we are using a special feature of our format which is the coloring because [pink is a **nice** color]{color=mypink}.

This is possible thanks to the Lua Filter included in the custom extension format.

